[
["index.html", "Particle Based Samplers for MCMC Chapter 1 Introduction to particle based sampler for MCMC 1.1 Assumed knowledge 1.2 Why would you use Particle Metropolis within Gibbs sampling? 1.3 The assumed hierarchical structure 1.4 What Particle Metropolis within Gibbs sampling provides 1.5 What is Particle Metropolis within Gibbs sampling? 1.6 Generating proposals in PMwG sampling using Particle Metropolis", " Particle Based Samplers for MCMC Jon-Paul Cavallaro Thursday 03 December 2020 Chapter 1 Introduction to particle based sampler for MCMC Contains implementations of particle based sampling methods for model parameter estimation. Primarily an implementation of the Particle Metropolis within Gibbs sampler outlined in the paper available here, it also contains code for covariance estimation. 1.1 Assumed knowledge To get the most out of this tutorial for the PMwG sampler, we assume you are familiar with hierarchical Bayesian estimation and MCMC sampling. If you would like to read more on these topics, please see Shiffrin et al. (2008) tutorial on Hierachical Bayesian Methods and Van Ravenzwaaij, Cassey, and Brown (2018) introduction to Markov Chain Monte-Carlo Sampling. 1.2 Why would you use Particle Metropolis within Gibbs sampling? This software is intended to help estimate models in a hierarchical structure with random effects for subjects. You will need to be able to write a function that evaluates the density of one subject’s data, given values for that subject’s parameters (i.e. their random effects). Everything else is taken care of by the sampler functions. The model that defines the density for an individual subject’s data could be a linear regression for example, or simple cognitive models like signal detection models (which is one of the examples we provide in this tutorial), or models that can be very challenging to estimate, such as the Linear Ballistic Accumulator (LBA) or Diffusion model. As long as you have a model for which you can provide a likelihood, this software will help estimate the model in a hierarchical Bayesian way. Benefits of the Particle Metropolis within Gibbs sampling algorithm include: It allows you to efficiently get posterior samples from difficult-to-estimate models with highly correlated parameters, such as the LBA or diffusion model, and these samples have nice properties (e.g., lower autocorrelation than other MCMC samplers). Statistical efficiency makes it feasilbe to draw a large number of posterior samples. This can be important in posterior inference, for example in calculating Bayes Factors using established methods. It allows you to estimate the covariance structure between parameters in a principled manner. 1.3 The assumed hierarchical structure The PMwG package is very flexible in that it is agnostic about the data-level model; it allows the user to specify the model that defines the density of each subject’s data. However, the package makes fixed assumptions about the hierarchical structure across participants. The package assumes a multivariate normal random effects structure. For example, when estimating an LBA model, each participant will have several parameters, such as a start point (A), threshold (b), drift rate (v), and non-decision time (t0). The PMwG package assumes that the vector of each subject’s parameter value follows a group-level distribution which is multivariate normal. The algorithm will estimate the group-level mean for each parameter, as well as its variance, and also the correlations between parameters in the sample. One consequence of the multivariate normal assumption is that all parameters are assumed to be unbounded (i.e. able to take values anywhere on the real line). Cognitive models often have bounded parameters (e.g. in the LBA model, the non-decision time parameter cannot be negative, as it represents a length of time). The user should deal with bounded parameters by transforming them to be unbounded. We give examples of that, in the likelihood functions. 1.4 What Particle Metropolis within Gibbs sampling provides The sampler will provide samples from the posterior distribution of the model in three categories: The means for the group level parameters (theta). The vectors of random effects for each subject (individual level parameter values, alpha) The group-level variance covariance matrix (sigma). 1.5 What is Particle Metropolis within Gibbs sampling? There are two sampling approaches incorporated into PMwG. One is the well-established and easy to apply Gibbs sampling on the group-level parameters. Gibbs sampling is very powerful and efficient, and it can work for the group-level parameters because the package assumes a multivariate normal distribution, which is easy to work with. However, for the subject-level parameters (random effects), Gibbs sampling is not possible; at least not for most cognitive models. For this reason, the PMwG package uses particle methods to sample random effects. Particle sampling works like other Markov chain samplers, such as Metropolis-Hastings. At each step, the sample (the vector of random effects) from the previous step is compared to a large number of proposals (“particles”). The new sample is chosen from amongst the particles (including the previous sample), according to how well they match the data and the prior. The key to making the algorithm efficient is to propose particles carefully. Our algorithm uses adaptive proposal distributions, individually tuned for each participant, to make sure that good proposal particles are generated without requiring a prohibitively large number of particles in total. How often the sampler accepts a new particle (as opposed the previous sample) is referred to as the acceptance rate. Acceptance rate can be adjusted for maximum efficiency (somewhere around 30-50% acceptance is great) by changing the total number of particles and by changing the variance of the proposal distribution (parameter “epsilon”). The particles are evaluated in parallel, which increases computation speed. 1.6 Generating proposals in PMwG sampling using Particle Metropolis PMwG has three sampling stages. The first stage is called burn-in, the second is adaptation, and the third is the sampled stage. The stages employ different numbers of particles, and different proposal distributions. This makes the algorithm most efficient. Figure 1.1: Trace plot with vertical lines demarcating the PMwG sampler’s three sampling stages In the burn-in and adaptation stages, the proposals (or random effects vectors) for each subject are sampled from a mixture of two sources. One source is the group-level distribution and the second source is a multivariate normal distribution centred on the current best guess for the subject’s random effects vector (alpha), with a variance that is smaller than the group level distribution. We generate proposals from both sources, because proposals generated from the group level distribution act as a safety net in situations where proposals generated from the subject’s current alpha are unusual or unlikely. If this occurs instead of the sampler taking a long way to return a sensible vector of random effects, a group level proposal will instead be chosen, leading to a faster sampling time. One thing we want to know is the posterior distribution for each subject’s random effects (alpha). For this reason, we throw away samples from the burn-in stage, because this is a period in which the sampler is trying to work its way away from an initial guess, and stabilise on samples which are from the true posterior distribution for each subjects random effects. In the adaptation stage, we continue the algorithm used in the burn-in stage until we collect a minimum of 20 unique samples from each subject’s posterior. These samples are used to give a good idea of what the posterior distribution looks like for each subject’s random effects vector. That allows us to build an adaptive proposal distribution that makes very efficient proposals, in the next stage. In the final sampling stage, we generate a multivariate normal distribution (referred to as the adaptive proposal distribution), that summarises the unique samples in the adaptation stage, and uses this to generate future proposals. An important feature of this distribution is that, for each subject, it summarises not only the posterior distribution of their random effects but also the way these random effects relate to the group-level parameter. This allows us to draw conditional proposals: proposals which are both consistent with that subject’s random effects and with the current proposal for the group-level distribution. Because of this, the proposals generated are frequently accepted, meaning we can lower the number of particles needed in this stage (for example, 20 instead of 200), and still maintain an adequate acceptance rate. Further, we continue to update this proposal distribution throughout the sampling stage so we have a more accurate proposal distribution. As a safety precaution during the sampling stage, we also include a few proposal particles from the same algorithm as used in the burn-in stage. This protects against very poor conditional proposal distributions. References "],
["pmwg-sampler-and-signal-detection-theory.html", "Chapter 2 PMwG sampler and Signal Detection Theory 2.1 Testing the SDT log-likelihood function 2.2 SDT log-likelihood function for Wagenmakers experiment 2.3 PMwG Framework 2.4 Check sampling process 2.5 Simulating posterior data", " Chapter 2 PMwG sampler and Signal Detection Theory Here we demonstrate how to use the PMwG sampler package to run a simple signal detection theory (SDT) analysis on data from a lexical decision task. We recognise that it is unnecessary to use the sampler package for a simple analysis such as this; however, we hope this SDT example demonstrates the practicality of the PMwG sampler package. 2.0.1 Signal Detection Theory analysis of lexical decision task We won’t cover SDT and lexical decision tasks (LDT) in detail here, however we will explain how you can use the PMwG package with SDT in the context of a lexical decision task. If you require more information, please visit the SDT and LDT wikipedia pages. Also, this Frontiers in Psychology tutorial paper by Anderson (2015) is another good resource for SDT. Participants were asked to indicate whether a letter string was a word or a non-word. We begin with the distributions for non-word and word stimuli. You can think of these two distributions as the ‘noise’ and ‘signal’ curves, respectively. Each distribution represents the evidence for ‘word-likeness’ and they are assumed to be normally distributed. The non-word distribution (or the ‘noise’ distribution) has a mean (\\(\\mu\\)) of 0 and a standard deviation (SD) of 1. We could estimate SD here, but we will use 1 in this example for simplicity. The mean for the word distribution is unknown at this point; however, we assign a d-prime (d’) parameter to denote the difference between the mean of the non-word and the mean of the word distributions (i.e. the ‘sensitivity’ to word stimuli or the signal-noise difference between words and non-word). As can be seen in Figure 2.1, the word distribution mean is greater than the non-word distribution mean; however, the distributions partially overlap where non-words and words are difficult to classify. Figure 2.1: Simple SDT example of lexical decision task The second parameter we denote is the criterion (C) parameter. The criterion is the point at which an individual responds non-word (to the left of C in Figure 2.2) or word (to the right of C in Figure 2.2) and it is set somewhere between the means of the two distributions. If you’re biased to respond word, the criterion would move to the left. Conversely, if you’re biased to respond non-word then the criterion would move to the right. Figure 2.2: Simple SDT example of lexical decision task 2.0.2 Writing a log-likelihood function Let’s write a simple log-likelihood function for a fabricated data set. You can copy the code below to follow along with the example. stim &lt;- c(&quot;word&quot;, &quot;word&quot;, &quot;non-word&quot;, &quot;word&quot;, &quot;non-word&quot;, &quot;non-word&quot;, &quot;non-word&quot;, &quot;word&quot;) resp &lt;- c(&quot;word&quot;, &quot;word&quot;, &quot;non-word&quot;, &quot;word&quot;, &quot;non-word&quot;, &quot;non-word&quot;, &quot;word&quot;, &quot;non-word&quot;) fab_data &lt;- as.data.frame(cbind(stim, resp)) We create our dataset by combining a response resp and a stimulus stim vector into a data frame as shown in 2.1 below. Table 2.1: A fabricated dataset of 8 trials with a response and a stimuls column stim resp word word word word non-word non-word word word non-word non-word non-word non-word non-word word word non-word Our log-likelihood function will step through the data, line by line, and find a likelihood value for each trial, under two parameters; d-prime d and criterion C. Here is our complete log-likelihood function. We have omitted some code from the code blocks below to enhance appearance, so we encourage you to copy the log-likelihood function from the following code block if you’d like to follow along with our example. We’ll now step through each component of the log-likelihood below. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 SDT_ll &lt;- function(x, data, sample = FALSE){ if (!sample) { out &lt;- numeric(nrow(data)) for (i in 1:nrow(data)) { if (stim[i] == &quot;word&quot;) { if (resp[i] == &quot;word&quot;) { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = x[&quot;d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = x[&quot;d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else { if (resp[i] == &quot;word&quot;) { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } } sum(out) } } We initialise the log-likelihood function with three arguments 1 SDT_ll &lt;- function(x, data, sample = FALSE) { x is a named parameter vector (e.g.pars) data is the dataset sample = sample values from the posterior distribution (For this simple example, we do not require a sample argument. ) The first if statement (line 2) checks if you want to sample, this is used for posterior predictive sampling which we will cover in later chapters. If you’re not sampling (like us in this example), you need to create an output vector out. The out vector will contain the log-likelihood value for each row/trial in your dataset. 2 3 4 5 6 if (!sample) { data$response &lt;- NA } else { out &lt;- numeric(nrow(data)) } From line 9, we check each row in the data set, first considering all trials with word stimuli if (stim[i] == &quot;word&quot; (line 10), and assign a likelihood for responding word (line 12-13) or non-word (line 15-16). The word distribution has a mean of x[&quot;d&quot;] (d-prime parameter) and a decision criterion parameter x[&quot;C&quot;]. If the response is word, we are considering values ABOVE or to the right of C in figure 2.2, so we set lower.tail = to FALSE. If the response is non-word, we look for values BELOW or to the left of C in figure 2.2 and we set lower.tail = to TRUE. The log.p = argument takes the log of all likelihood values when set to TRUE. We do this so we can sum all likelihoods at the end of the log-likelihood function. 8 9 10 11 12 13 14 15 16 17 if (!sample) { for (i in 1:nrow(data)) { if (stim[i] == &quot;word&quot;) { if (resp[i] == &quot;word&quot;) { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = x[&quot;d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = x[&quot;d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } From the else statement on line 18, we have the function for non-word trials i.e. stim[i] == &quot;non-word&quot;. As can be seen below, the output value out[i] for these trials is arrived at in a similar manner to the word trials. We set the mean to 0 and the standard deviation sd to 1. If the response is word, we are considering values ABOVE or to the right of C in figure 2.2, so we set lower.tail = to FALSE. If the response is non-word, we look for values BELOW or to the left of C in figure 2.2 and we set lower.tail = to TRUE. Again we want the log of all likelihood values so we set log.p = TRUE. 18 19 20 21 22 23 24 25 26 else { if (resp[i] == &quot;word&quot;) { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } The final line of code on line 24 sums the out vector and returns a log-likelihood value for your model. sum(out) 2.1 Testing the SDT log-likelihood function Before we run the log-likelihood function, we must create a parameter vector pars containing the same parameter names used in our log-likelihood function above i.e. we name the criterion C and d-prime parameter d. While we’re testing the log-likelihood, we assign arbitrary values to each parameter. pars &lt;- c(C = 0.8, d = 2) We can test run our log-likelihood function by passing the parameter vector pars and the fabricated dataset we created above fab_data. SDT_ll(pars, fab_data) ## [1] -4.795029 Now, if we change the parameter values, the log-likelihood value should also change. pars &lt;- replace(pars, c(1,2), c(0.5, 1.2)) SDT_ll(pars, fab_data) ## [1] -4.532791 We can see the log-likelihood has changed. The second vector of parameter values are more likely than the first vector given the data. 2.2 SDT log-likelihood function for Wagenmakers experiment Now that we’ve covered a simple test example, let’s create a log-likelihood function for the Wagenmakers et al. (2008) dataset. 2.2.1 Description of Wagenmakers experiment If you’d like to follow our example, you will need to download the dataset from this link.The structure of the dataset will need to be modified in order to meet the requirements of the PMwG sampler. To do this, you can copy our script. You may attempt to modify the dataset yourself, but you must recreate the structure illustrated in table 2.2. Participants were asked to indicate whether a letter string was a word or a non-word. A subset of Wagenmakers et al data are shown in table 2.2, with each line representing a single trial. We have a subject column with a subject id (1-19), a condition column cond which indicates the proportion of words to non-words presented within a block of trials. In word blocks (cond = w) participants completed 75% word and 25% non-word trials and for non-word (cond = nw) blocks 75% non-word and 25% word trials. The stim column lists the word’s frequency i.e. is the stimulus a very low frequency word (stim = vlf), a low frequency word (stim = lf), a high frequency word (stim = hf) or a non-word (stim = nw). The third column resp refers to the participant’s response i.e. the participant responded word (resp = W) or non-word (resp = NW). The two remaining columns list the response time (rt) and whether the paricipant made a correct (correct = 2) or incorrect (correct = 1) choice. For more details about the experiment please see the original paper. Table 2.2: Subset of 12 trials from the Wagenmakers (2008) dataset. subject cond stim resp rt correct 1 w lf W 0.410 2 1 w hf W 0.426 2 1 w nw NW 0.499 2 1 w lf W 0.392 2 1 w vlf W 0.435 2 1 w hf W 0.376 2 1 nw lf W 0.861 2 1 nw hf W 0.563 2 1 nw nw NW 0.666 2 1 nw nw NW 1.561 2 1 nw nw NW 0.503 2 1 nw nw NW 0.445 2 Our log-likelihood function for Wagenmakers experimental data is similar to the function we wrote above, except now we require a criterion parameter for each condition and a d-prime parameter for each of the stim word types. This is illustrated in figure 2.3 below, where we have a non-word criterion Cnw, a word criterion Cw and three distributions for each of the stim types with corresponding d-prime for each distribution: dvlf, dlf, dhf. Figure 2.3: Signal detection theory example of lexical decision task Here is our complete log-likelihood function for the Wagenmakers data set. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 SDT_ll &lt;- function(x, data, sample = FALSE){ if (sample){ data$response &lt;- NA } else { out &lt;- numeric(nrow(data)) } if (!sample){ for (i in 1:nrow(data)) { if (data$cond[i] == &quot;w&quot;) { if (data$stim[i] == &quot;hf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;lf&quot;){ if (data$resp[i] == &quot;W&quot;){ out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;vlf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } } else { if (data$stim[i] == &quot;hf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;lf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;vlf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.nw&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } } } sum(out) } } Line 1 through to line 8 are the same as the log-likelihood we wrote for the fabricated dataset above. From line 9, we calculate the log-likelihood out[i] for word condition trials cond[i] == &quot;w&quot; when the stimulus is a high frequency word stim[i] == &quot;hf&quot; for each response. We do this by considering the upper tail of the high frequency word distribution lower.tail = FALSE, from the word criterion Cw, for word responses resp[i] == &quot;W&quot; and the lower tail for non-word responses (else statement on line 14). We then recycle this process for the remaining conditions/parameters in the experiment. 9 10 11 12 13 14 15 16 17 if (data$cond[i] == &quot;w&quot;) { if (data$stim[i] == &quot;hf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } This give us a log-likelihood for all data. Let’s test this… pars &lt;- log(c(C.w = 1, C.nw = 0.5, HF.d = 3, LF.d = 1.8, VLF.d = 0.7)) SDT_ll(pars, wgnmks2008, sample = FALSE) ## [1] -30801.71 2.2.2 Computation time of log-likelihood function You may have noticed that our log-likelihood function is slow and heavy on computer time when processing the data trial by trial. We recommend you write a ‘slow’ log-likelihood (as written above) to check it functions as it should before improving the function’s efficiency. Now we’ll speed up our log-likelihood function. We have 16 possible values that could be assigned per line in the previous function (for the 16 cells of the design given by proportion (2) x stimuli (4) x response (2)). Rather than looping over each trial, we could calculate the log-likelihood for each cell in the design and multiply the number of instances for each subject. To do this, we add a column to the dataframe by tabling as shown in the code below wgnmks2008Fast &lt;- as.data.frame(table(wgnmks2008$subject, wgnmks2008$cond, wgnmks2008$stim, wgnmks2008$resp)) names(wgnmks2008Fast) &lt;- c(&quot;subject&quot;, &quot;cond&quot;, &quot;stim&quot;, &quot;resp&quot;, &quot;n&quot;) Now our data frame looks like this.. subject cond stim resp n 1 nw hf NW 1 2 nw hf NW 2 3 nw hf NW 11 4 nw hf NW 0 5 nw hf NW 6 6 nw hf NW 9 For our SDT log-likelihood function, we add n* (i.e. a multiplying factor) to each of these values to calculate the model log-likelihood and we no longer loop over trials, otherwise the log-likelihoods are the same. SDT_ll_fast &lt;- function(x, data, sample = FALSE) { if (!sample) { out &lt;- numeric(nrow(data)) for (i in 1:nrow(data)) { if (data$cond[i] == &quot;w&quot;) { if (data$stim[i] == &quot;hf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;lf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;vlf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.w&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } }else{ if (data$stim[i] == &quot;hf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;HF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;lf&quot;){ if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;LF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else if (data$stim[i] == &quot;vlf&quot;) { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = x[&quot;VLF.d&quot;], sd = 1, log.p = TRUE, lower.tail = TRUE) } } else { if (data$resp[i] == &quot;W&quot;) { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = FALSE) } else { out[i] &lt;- data$n[i] * pnorm(x[&quot;C.nw&quot;], mean = 0, sd = 1, log.p = TRUE, lower.tail = TRUE) } } } } sum(out) } } Now we have a fast(er) SDT log-likelihood function and we can compare its output with the slow log-likelihood function’s output to make sure it is functioning correctly. pars &lt;- log(c(C.w = 1, C.nw = 0.5, HF.d = 3, LF.d = 1.8, VLF.d = 0.7)) SDT_ll(pars, wgnmks2008, sample = FALSE) ## [1] -30801.71 SDT_ll_fast(pars, wgnmks2008Fast, sample = FALSE) ## [1] -30801.71 Great—both functions produce the same log-likelihood! And we can run one final check by modifying the parameter vector’s values pars &lt;- log(c(C.w = 1, C.nw = 0.8, HF.d = 2.7, LF.d = 1.8, VLF.d = 1.3)) SDT_ll(pars, wgnmks2008, sample = FALSE) ## [1] -22168.95 SDT_ll_fast(pars, wgnmks2008Fast, sample = FALSE) ## [1] -22168.95 We recommend speeding up your code however you wish. When you’re confident that your log-likelihood functions correctly, you should save it as a separate script so it can be sourced and loaded when running the sampler. 2.3 PMwG Framework Now that we have written a log-likelihood function, we’re ready to use the PMwG sampler package. Let’s begin by loading the PMwG package. library(pmwg) Now we require the parameter vector pars we specified above and a priors object called priors. The priors object is a list that contains two components: theta_mu_mean a vector that is the prior for the mean of the group-level mean parameters theta_mu_var a covariance matrix that is the prior for the variance of the group-level mean parameters. In all examples we assume a diagonal matrix. pars &lt;- c(&quot;C.w&quot;, &quot;C.nw&quot;, &quot;HF.d&quot;, &quot;LF.d&quot;, &quot;VLF.d&quot;) # This is the same as the `pars` vector specified above priors &lt;- list( theta_mu_mean = rep(0, length(pars)), theta_mu_var = diag(rep(1, length(pars))) ) The priors object in our example is initiated with zeros. An important thing to note is that to facilitate Gibbs sampling from the multivariate normal distribution for the group parameters, the random effects must be estimated on the real line. In our SDT example, the parameters are free to vary along the real line so no transformation of the random effects is required. We expand on this point in more detail in later examples where parameters are bounded and require transformation. The prior on the covariance matrix is hard-coded as the marginally non-informative prior of Huang and Wand, as discussed in the PMwG paper. The next step is to load your log-likelihood function/script. source(file = &quot;yourLogLikelihoodFile.R&quot;) Once you have setup your parameters, priors and written a log-likelihood function, the next step is to initialise the sampler object. sampler &lt;- pmwgs(data = wgnmks2008Fast, pars = pars, prior = priors, ll_func = SDT_ll_fast ) The pmwgs function takes a set of arguments (listed below) and returns a list containing the required components for performing the particle metropolis within Gibbs steps. data =a data frame (e.g.wgnmks2008Fast) with a column for participants called subject pars = the model parameters to be used (e.g.pars) prior = the priors to be used (e.g.priors) ll_func = name of log-likelihood function you’ve sourced above (e.g.SDT_ll_fast) sampler &lt;- pmwgs(data = wgnmks2008Fast, pars = pars, prior = priors, ll_func = SDT_ll_fast ) 2.3.1 Model start points You have the option to set model start points. We use 0 for the mean (mu) and a variance (sig2) of 0.01. If you chose not to specify start points, the sampler will randomly sample points from the prior distribution. start_points &lt;- list(mu = rep(0, length.out = length(pars)), sig2 = diag(rep(.01, length(pars))) ) The start_points object contains two vectors: mu a vector of start points for the mean the model parameters sig2 vector containing the start points of the covariance matrix of covariance between model parameters. 2.3.2 Running the sampler Okay - now we are ready to run the sampler. sampler &lt;- init(sampler, start_mu = start_points$mu, start_sig = start_points$sig2) Here we are using the init function to generate initial start points for the random effects and storing them in the sampler object. First we pass the sampler object from above that includes our data, parameters, priors and log-likelihood function. If we decided to specify our own start points (as above), we would include the start_mu and start_sig arguments. Now we can run the sampler using the run_stage function. The run_stage function takes four arguments: x the sampler object including parameters that was created from the init function above. stage = the sampling stage (e.g. &quot;burn&quot;, &quot;adapt&quot; or &quot;sample&quot;) iter = is the number of iterations for the sampling stage. This is similar to running deMCMC, where it takes many iterations to reach the posterior space. Default = 1000. particles = is the number of particles generated on each iteration. Default = 1000. display_progress = shows progress bar for current stage n_cores = the number of cores to be used when running the sampler epsilon = is a value between 0 and 1 which reduces the size of the sampling space. We use lower values of epsilon when there are more parameters to estimate. Default = 1. It is optional to include the iter = and particles = arguments. If these are not included, iter and particles default to 1000. The number of iterations you choose for your burn in stage is similar to choices made when running deMCMC, however, this varies depending on the time the model takes to reach the ‘real’ posterior space. First we run our burn-in stage by setting stage = to &quot;burn&quot;. burned &lt;- run_stage(sampler, stage = &quot;burn&quot;, iter = 1000, particles = 20, display_progress = TRUE, n_cores = 8) Now we run our adaptation stage by setting stage = &quot;adapt&quot;. This function creates an efficient proposal distribution. The sampler will attempt to create the proposal distribution after 20 unique particles have been accepted for each subject. The sampler will then test whether the distribution was able to be created and if it was created, the sampler will move to the next stage otherwise the sampler will continue to sample. The number of iterations needs to be great enough to generate enough unique samples. The sampler will automatically exit the adapt stage when it has enough unique samples to create a multivariate normal ‘proposal’ distribution for each subject’s random effects. Thus we set iterations to a high number, as it should exit before reaching this point. adapted &lt;- run_stage(burned, stage = &quot;adapt&quot;, iter = 1000, particles = 20, n_cores = 8) At the start of the sampled stage, the sampler object will create a ‘proposal’ distribution for each subject’s random effects using a conditional multi-variate normal. This proposal distribution is then used to efficiently generate new particles for each subject which means we can reduce the number of particles on each iteration whilst still achieving acceptance rates. sampled &lt;- run_stage(adapted, stage = &quot;sample&quot;, iter = 1000, particles = 20, n_cores = 8) 2.4 Check sampling process It is a good idea to check your samples by producing some simple plots as shown below. The first plot gives an indication of the trace for the group level parameters. In this example, you will see the chains take only several iterations before arriving at the posterior, however, this may not always be the case. Each parameter trace (lines on plot 2.4) should be stationary i.e. the trace should not trend up or down, and once the sampler reaches the posterior, the trace should remain relatively ‘thin’. If the trace is wide and bounces between large values (e.g. between -3 and 3) then there may be an error in your log-likelihood function. As you can see in 2.4, the traces are clearly stable. Note that the number of iterations for the adaptation stage here is quite short because it’s easy to estimate and so gets lots of unique draws. Figure 2.4: Posterior samples of parameters The second plot below (figure 2.5) shows the likelihoods across iterations for each subject. Again we see that the likelihood values jump up after only a few iterations and then remain stable, with only slight movement. Figure 2.5: Posterior samples of subject log-likelihoods 2.5 Simulating posterior data Now we’ll cover the sample operation within the fast log-likelihood function. We will use this on the full data set. The sample operation can be carried out in several ways (using rbinom etc). Please note that we do NOT recommend using this approach below and this should serve as an example only. The sample process is similar to what we’ve covered above. We begin by assigning NAs to the response column to prepare it for simulated response data. We then consider a subset of the data, beginning with word condition and high-frequency hf word stimuli trials. else{ data$resp &lt;- NA for (i in 1:nrow(data)){ if (data$cond[i] == &quot;w&quot;){ if (data$stim[i] == &quot;hf&quot;){ We then take the criterion for the word condition i.e. C.w. To simulate a response given our parameters we use rnorm to pick a random value from a normal distribution with mean = HF.d (i.e. high frequency word stimulus) and a SD of 1 and we test that value against the word criterion C.w. If the value is larger than C.w, the simulated response will be word otherwise, the simulated response will be non-word. data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;HF.d&quot;], sd = 1)) &gt; x[&quot;C.w&quot;], &quot;word&quot;, &quot;non-word&quot;) We repeat this process for each condition and stimulus combination as shown in the code block below. { else if (data$stim[i] == &quot;lf&quot;) { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;LF.d&quot;], sd = 1)) &gt; x[&quot;C.w&quot;], &quot;word&quot;, &quot;non-word&quot;) } else if (data$stim[i] == &quot;vlf&quot;) { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;VLF.d&quot;], sd = 1)) &gt; x[&quot;C.w&quot;], &quot;word&quot;, &quot;non-word&quot;) } else { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = 0, sd = 1)) &gt; x[&quot;C.w&quot;], &quot;word&quot;, &quot;non-word&quot;) } } else { if (data$stim[i] == &quot;hf&quot;) { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;HF.d&quot;], sd = 1)) &gt; x[&quot;C.nw&quot;], &quot;word&quot;, &quot;non-word&quot;) } else if (data$stim[i] == &quot;lf&quot;) { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;LF.d&quot;], sd = 1)) &gt; x[&quot;C.nw&quot;], &quot;word&quot;, &quot;non-word&quot;) } else if (data$stim[i] == &quot;vlf&quot;) { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = x[&quot;VLF.d&quot;], sd = 1)) &gt; x[&quot;C.nw&quot;], &quot;word&quot;, &quot;non-word&quot;) } else { data$resp[i] &lt;- ifelse(test = (rnorm(1, mean = 0, sd = 1)) &gt; x[&quot;C.nw&quot;], &quot;word&quot;, &quot;non-word&quot;) } } Now we can run our simulation. Below is some code to achieve this. n.posterior &lt;- 20 # Number of samples from posterior distribution for each parameter. pp.data &lt;- list() S &lt;- unique(wgnmks2008$subject) data &lt;- split(x = wgnmks2008, f = wgnmks2008$subject) for (s in S) { cat(s,&quot; &quot;) iterations = round(seq(from = 1051, to = sampled$samples$idx, length.out = n.posterior)) for (i in 1:length(iterations)) { x &lt;- sampled$samples$alpha[, s, iterations[i]] names(x) &lt;- pars tmp &lt;- SDT_ll_fast(x = x, data = wgnmks2008[wgnmks2008$subject == s,], sample = TRUE) if (i == 1) { pp.data[[s]] &lt;- cbind(i,tmp) } else { pp.data[[s]] &lt;- rbind(pp.data[[s]], cbind(i, tmp)) } } } And now we can plot samples against the data. Figure 2.6: The proportion of ‘word’ responses within each cell of the design. Figure 2.6 shows 20 posterior draws (dots) plotted against the data (bars). The posterior draws are for each individual subject - shown here is the average proportion of word responses. We can see that the model fits the data well. References "],
["forstmannChapter.html", "Chapter 3 PMwG sampler and sequential sampling models 3.1 The speed-accuracy tradeoff in perceptual decisions 3.2 Linear Ballistic Accumulator Parameters 3.3 Writing the log-likelihood function 3.4 PMwG Framework 3.5 Simulating Posterior Predictive Data 3.6 Evaluating different models - single threshold LBA 3.7 Checking Descriptive Adequacy of 1b model. 3.8 Model Comparison 3.9 Checking the LBA log-likelihood function", " Chapter 3 PMwG sampler and sequential sampling models In this chapter we’ll demonstrate how to use the PMwG sampler with a sequential sampling model; the Linear Ballistic Accumulator (LBA). Please ensure the PMwG and rtdists packages are loaded. library(pmwg) library(rtdists) 3.1 The speed-accuracy tradeoff in perceptual decisions We demonstrate the application of the LBA with the PMwG sampler to a study of perceptual decision making (Forstmann et al., 2008). Forstmann et al looked at neural correlates of decision making under time pressure, with an aim to identify areas of the brain associated with speed-accuracy tradeoff. Imaging (fMRI) and behavioural data was collected; however, we will analyse behavioural data from the decision-making task only. In terms of modelling the data, Forstmann expected to find differences in thresholds for each of the three speed-emphasis conditions. We have included the Forstmann et als data in the PMwG package as a data frame named forstmann. The sampler requires a data frame with a subject column. The subject column data type can be a factor or numeric. Table 3.1 shows the first ten trials from the Forstmann dataset. Participants (n = 19) were asked to indicate whether a cloud of dots in a random-dot kinematogram (RDK) moved to the left or the right of the screen. The IV was a within-subject, speed-accuracy manipulation where, before each trial began, pariticipants were instructed to make their choice accurately (condition = 1), with urgency(condition = 3)or were presented with a neutral message (condition = 2). Stimuli moved either left (stim = 1) or right (stim = 2) and responses were left (resp = 1) or right (resp = 2). Response times (rt) were recorded in seconds. For more information about the design of the experiment please see the original paper. Table 3.1: First 10 trials in Forstmann dataset. The forstmann dataset is an data frame subject condition stim resp rt 1 1 2 2 0.4319 1 3 2 2 0.5015 1 3 1 1 0.3104 1 1 2 2 0.4809 1 1 1 1 0.3415 1 2 1 1 0.3465 1 2 1 1 0.3572 1 2 2 2 0.4042 1 2 1 1 0.3866 1 1 2 2 0.3683 3.2 Linear Ballistic Accumulator Parameters There are preliminary steps we need to complete before running the sampler. Let’s begin by defining the Linear Ballistic Accumulator (LBA) (S. Brown and Heathcote 2008) model parameters. b threshold parameter (the evidence required to trigger a response) v is the mean drift rate or average speed of evidence accumulation A is the range of start points for accumulators t0 is non-decision time sv is the standard deviation of the across-trial distribution of drift rates 3.3 Writing the log-likelihood function Just as we did with the SDT example, we’ll write a slow and a fast log-likelihood function. The runtime difference is caused by calling the dLBA function line-by-line for the slow log-likelihood and calling the dLBA function once for all the data in the fast log-likelihood function. When writing a new log-likelihood function, we suggest starting with a slow, line-by-line function for easier debugging. The LBA log-likelihood function takes three arguments: x is a named parameter vector (e.g. pars) data is your dataset (e.g.forstmann). Your dataset must include a &quot;subject&quot; column sample = FALSE calculates a density function or TRUE generates a posterior predictive sample that matches the shape of data. The log-likelihood function shown below includes functions from the rtdists package for generating data and estimating density. If you’d like to run through this example, it is best to copy the tw_lba_ll function from the code block below rather than copying from the separate code chunks where curly braces have been removed. NOTE: The trialwise log-likelihood is very slow and inneficient because rLBA and dLBA will be called on each line of the data. This will result in very slow sampling times and is a consequence of the rtdists package, not an issue with the PMwG sampling speed. If you have experience writing log-likelihoods, we recommend writing a faster version than our trialwise function, or use the fast log-likelihood we have written in section 3.3.1. If you are new to modelling, we recommend trying the trialwise (slow) log-likelihood function as it is easier to follow, troubleshoot and is less likely to result in errors. Let’s begin by loading the rtdists package… library(rtdists) and now our complete trialwise (slow) log-likelihood function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 tw_lba_ll &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } if (sample) { tmp &lt;- numeric(nrow(data)) data$rt &lt;- rep(NA, nrow(data)) data$resp &lt;- rep(NA, nrow(data)) } else { out &lt;- numeric(nrow(data)) } for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b = x[paste0(&quot;b.&quot;, data$condition[i])] + A vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] t0 = x[&quot;t0&quot;] s = c(1, 1) if (data$stim[i] == 1) { vs = c(vc, ve) } else { vs = c(ve, vc) } if (sample) { tmp &lt;- rLBA(n = 1, A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } else { out[i] &lt;- dLBA(rt = data$rt[i], response = data$resp[i], A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) } } if (sample) { return(data) } else { bad &lt;- (out &lt; 1e-10) | (!is.finite(out)) out[bad] &lt;- 1e-10 out &lt;- sum(log(out)) return(out) } } The first line in the tw_lba_ll function (Line 2 below) takes the exponent of the parameter values. We do this as the LBA requires positive parameter values that are on the real line. Line 3 and 4 then checks RTs are faster than the non-decision time parameter t0, and returns a low value if t0 is larger than RT, indicating that the given value of t0 is unlikely. 2 3 4 x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) Now we create a vector with values sampled from the posterior distribution OR estimating the density. If sample = TRUE, we remove all responses (resp) and rts. This means when we return data, we are returning the posterior predictive data which matches with the associated subject and condition and then generate them from the random function of the model. If sample = FALSE (the else statement from line 11) we create an out vector, with a length equal to the number of rows in the dataset, and store the likelihood value for each subject and condition. 7 8 9 10 11 12 13 if (sample) { tmp &lt;- numeric(nrow(data)) data$rt &lt;- rep(NA, nrow(data)) data$resp &lt;- rep(NA, nrow(data)) } else { out &lt;- numeric(nrow(data)) } Next, we loop over rows in the dataset. In this for loop, we find the values (x) of each parameter in our model for each row, so that any conditional parameters (for example b in our model) are correctly assigned. For example, we want to calculate the density for a model that has three threshold parameters (one for each condition; 1 = accuracy, 2 = neutral, or 3 = speed). In the loop, we paste b. to the condition in row [i] and add A (the start point - we do this to ensure the threshold is greater than the starting point). On line 22 we set the order of our drift rate parameters. Recall that stim = 1 is a stimulus moving to the left. dLBA requires the drift accumulators to be matched i.e. when data$stim[i] == 1, the drift rate for the correct accumulator (vc) is in position one, so we order the drift rates; vs = c(vc, ve). The else statement addresses right moving stimuli data$stim[i] == 2, the incorrect accumulator (ve) is the first accumulator, so the drift rate parameter order is vs = c(ve, vc). This ensures that the correct (vc) and error (ve) drift rates match with the corresponding accumulators for given stimuli. 15 16 17 18 19 20 21 22 23 24 25 26 for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b = x[paste0(&quot;b.&quot;, data$condition[i])] + A vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] t0 = x[&quot;t0&quot;] s = c(1, 1) if (data$stim[i] == 1) { vs = c(vc, ve) } else { vs = c(ve, vc) } The following section calls the relevant rtdists function depending on whether we are sampling from the posterior predictive distribution (rLBA) or estimating the density (dLBA). We then input the parameters from above (using the names set above) into the relevant function. When generating data from the posterior predictive distribution (Line 30-37), rLBA is called for each line of the data, storing the generated rt and response given the posterior parameter estimates in the tmp vector (which we then reassign to the empty data$rt and data$resp columns). We set n = 1, since we are calling rLBA on 1 row of the data. When estimating the density (Line 42 to 50), dLBA is called for each line of the data, storing the probability of the rt and response under the proposed parameters (x) in the out vector. 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 if (sample) { tmp &lt;- rLBA(n = 1, A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } else { out[i] &lt;- dLBA(rt = data$rt[i], response = data$resp[i], A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) } This final section tells the function what to return; data - when sampling posterior predictive data (Line 56) - or the sum of the likelihoods - when estimating density (Line 57 - 61). On line 58 we take all implausible likelihood values, assign them to the bad object and then (line 59) set them to an extremely unlikely value, to prevent numerical errors. The final two lines within the else statement take the log of all likelihood values, sums them, assigns the model’s log-likelihood value to the out variable and returns that value. 55 56 57 58 59 60 61 62 if (sample) { return(data) } else { bad &lt;- (out &lt; 1e-10) | (!is.finite(out)) out[bad] &lt;- 1e-10 out &lt;- sum(log(out)) return(out) } 3.3.1 Fast LBA Log-likelihood Function As the data is large, and the dLBA function takes some time to run, the log-likelihood code above is computationally inefficient. There are several ways to improve the log-likelihood’s performance; in our example below, we reduce the number of calls to dLBA to one call. We do this by passing a list of dLBA parameter values for the length of the data. Note: When generating posterior predictive data, the rLBA function is still executed for each row of data; however, it is only executed several times, so computational efficiency is uncompromised. fast_lba_ll3b &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } if (sample) { data$rt &lt;- rep(NA, nrow(data)) data$resp &lt;- rep(NA, nrow(data)) } else { out &lt;- numeric(nrow(data)) } if (sample) { for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b = x[paste0(&quot;b.&quot;, data$condition[i])] + A vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] t0 = x[&quot;t0&quot;] s = c(1, 1) if (data$stim[i] == 1) { vs = c(vc, ve) } else { vs = c(ve, vc) } tmp &lt;- rLBA(n = 1, A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } } else { all_b &lt;- numeric(nrow(data)) vlist &lt;- list(&quot;v.1&quot; = numeric(nrow(data)), &quot;v.2&quot; = numeric(nrow(data))) stim &lt;- levels(data$stim) con &lt;- levels(data$condition) for (c in con) { for (s in stim) { use &lt;- data$condition == c &amp; data$stim == s if (any(use)) { bs = x[paste0(&quot;b.&quot;, c)] + x[&quot;A&quot;] all_b[use] = bs vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] if (s == 1) { vlist$v.1[use] = vc vlist$v.2[use] = ve } else { vlist$v.1[use] = ve vlist$v.2[use] = vc } } } } out &lt;- dLBA(rt = data$rt, response = data$resp, A = x[&quot;A&quot;], b = all_b, mean_v = vlist, sd_v = c(1, 1), t0 = x[&quot;t0&quot;], distribution = &quot;norm&quot;, silent = TRUE ) } if (sample) { return(data) } else { bad &lt;- (out &lt; 1e-10) | (!is.finite(out)) out[bad] &lt;- 1e-10 out &lt;- sum(log(out)) return(out) } } You should improve your log-likelihood’s performance as you see fit. When you’re confident that your log-likelihood code functions correctly, we suggest saving it as a separate script so it can be sourced and loaded when running the sampler. If you’re learning how to write log-likelihood functions, take a look at our troubleshooting section for tips. In our experience, a very large proportion of problems with sampling and inference are caused by inadequate checking and care in the likelihood function. 3.4 PMwG Framework Now that we have a log-likelihood function, we can set up the PMwG sampler. Running the sampler follows the same procedure outlined in the SDT chapter; we need to set up a vector of model parameter names, create a priors object, source our LBA log-likelihood script and then create our sampler object. Let’s begin by creating a vector of model parameter names, which we’ll use in our log-likelihood function. You can name this object as you wish; however, in our example, we name it pars.1 For the forstmann dataset, we use three threshold parameters (one b for each condition) because we assume that the condition has an effect on level of caution, we include two drift rate parameters: ve for the incorrect accumulator and vc for the correct accumulator, a start point parameter A and a non-decision time t0 parameter. We’ve made a decision to set the sv to 1 to satisfy the scaling properties of the model. As such, we haven’t included the sv parameter in the pars vector - it is found in the LBA’s log-likelihood function (see above). pars &lt;- c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;A&quot;, &quot;ve&quot;, &quot;vc&quot;, &quot;t0&quot;) For the mean of the distribution for random effects (theta_mu), we assume a multivariate normal prior. The user can specify the mean and variance of this prior distribution using the object priors, which has elements theta_mu_mean and theta_mu_var. A typical setting for LBA models is to set theta_mu_mean to be a zero vector and to set theta_mu_var to be a multiple of the identity matrix, e.g. with 9 on the diagonal (representing a standard deviation of 3 for the subject-level means in the prior). We create our priors object; a list that contains two components: theta_mu_mean a vector containing the prior for model parameter means theta_mu_var the prior covariance matrix for model parameters. priors &lt;- list(theta_mu_mean = rep(0, length(pars)), theat_mu_var = diag(rep(1, length(pars))) ) Now source and load your log-likelihood script before you create the sampler object. source(file = &quot;fast_lba_ll3b.R&quot;) Next we specify the PMwG sampler object. The pmwgs function takes a set of arguments (listed below) and returns a list containing the required components for performing the particle metropolis within Gibbs steps. data = your data - a data frame (e.g.forstmann) with a column for participants called subject pars = the model parameters to be used (e.g.pars) prior = the priors to be used (e.g.priors) ll_func = name of log-likelihood function to be used (e.g.fast_lba_ll) sampler &lt;- pmwgs(data = data, pars = pars, prior = priors, ll_func = fast_lba_ll3b ) 3.4.1 Model start points There is also an option to set model start points. We have specified sensible start points for the forstmann dataset under the LBA model. If you choose not to specify start points, the sampler will randomly sample points from the prior distribution. The start_points object contains two vectors: mu a vector of start points for the mu of each model parameter sig2 vector containing the start points of the covariance matrix of covariance between model parameters. Note: Start points must be on the real line. Our log-likelihood function immediately takes the exponent of the start points and only returns positive values, so we use the log of sensible start points here. start_points &lt;- list(mu = log(c(1.2, 1.2, 1.2, 1.4, 1.3, 3.5, 0.13) ), sig2 = diag(rep(.01, length(pars))) ) 3.4.2 Running the sampler Setup is now complete and we can run the sampler. First, we use the init function to generate initial start points for the random effects and store them in the sampler object. Here we specify start points by providing values for the start_mu and start_sig arguments. sampler &lt;- init(sampler, start_mu = start_points$mu, start_sig = start_points$sig2 ) To run the sampler, we use the run_stage function. To execute the run_stage function, you must provide values for two arguments: pmwgs = the sampler object including parameters that were created by the init function above. stage = the sampling stage (In order; &quot;burn&quot;, &quot;adapt&quot; or &quot;sample&quot;). The following arguments listed below are optional: iter = is the number of iterations for the sampling stage. For burn-in, it is important to have enough iterations that the chains converge on the posterior distribution. Default = 1000. particles = is the number of proposals (particles) generated for each random effect, on each iteration. Default = 1000, but set smaller or larger in order to target a reasonable acceptance rate (i.e. 10-60%). display_progress = display a progress bar during sampling epsilon = is a value greater than 0 which scales the variance of the proposal distribution. Smaller values (i.e. narrower proposal distributions) can lead to higher acceptance rates, but slower coverage of the posterior. Smaller values are especially useful when the number of random effects is large (e.g. &gt;10). Default = 1. n_cores = the number of cores on a machine you wish to use to run the sampler. This allows sampling to be run across cores (parallelising for subjects). Default = 1. Note: Setting n_cores greater than 1 is only permitted on Linux and Mac OS X machines. The first sampling stage is burn-in &quot;burn&quot;. The burn-in stage allows time for the sampler to move from the (arbitrary) start points that were provided by the user to the mode of the posterior distribution. This can be checked by examining the chains for stationarity. We take the sampler object created in the init function above, set the stage argument to &quot;burn&quot; and assign the outcome to an object called burned. burned &lt;- run_stage(sampler, stage = &quot;burn&quot;, iter = 500, particles = 2000, epsilon = .5 ) Next is the adaptation stage &quot;adapt&quot;. The adaptation stage draws samples using a simple, but relatively inefficient proposal distribution (the same proposal distribution as the &quot;burn&quot;stage). Enough samples are drawn to allow the algorithm to estimate a much more sophisticated and efficient proposal distribution, using conditional normal distributions. We take the burned object created in the previous stage and set iterations iter = to a high number (e.g. 10000), as it should exit before reaching this point. If it doesn’t, there is likely an issue with acceptance rates, the likelihood function or limited data to operate on (i.e. few trials in some conditions). Here, we have saved the outcome of the adaptation stage to an object called adapted. adapted &lt;- run_stage(burned, stage = &quot;adapt&quot;, iter = 10000, particles = 2000, epsilon = .5 ) The final stage is the sampling stage &quot;sample&quot;. The sampling stage uses the sophisticated and adaptive conditional normal proposal distributions. This allows for very efficient sampling, using far fewer particles. Samples from this stage are taken from the ‘posterior distribution’ and stored in the sampled object. sampled &lt;- run_stage(adapted, stage = &quot;sample&quot;, iter = 1000, particles = 200, epsilon = .5 ) The sampled object includes all samples from the &quot;sample&quot; stage above and the following elements: data : your data (data frame) you included in your analysis par_names: parameter names n_pars: number of parameters n_subjects: number of subjects subjects: subject IDs (1:n) prior: list of the prior used ll_func: the likelihood function specified samples: alpha: three dimensional array of random effects draws (dim = parameters x subjects x samples) theta_mu: two dimensional array of parameter draws (dim = parameters x samples) theta_sig: three dimensional array of covariance matrix draws (dim = covariance x samples) stage: specifies the stage the sample is from (length = samples) subj_ll: likelihood value for each subject for each iteration (dim = subject x samples) a_half: the parameter used in calculating the inverse Wishart (dim = parameters x samples) idx: total number of samples last_theta_sig_inv: the inverse of the last sample for theta_sig (the variance-covariance matrix). You should save your sampled object at this point. save(sampled, file = &quot;forst3bSamp.RData&quot;) 3.5 Simulating Posterior Predictive Data We can generate posterior predictive data by setting sample = TRUE in our log-likelihood function to generate response times and responses given the posterior parameter estimates for each subject. To do this, we use the gen_pp_data function below, which calls our log-likelihood function embedded in our sampled object. The gen_pp_data function takes four arguments: sampled: is the object/output from the PMwG sampler n: the number of posterior samples ll_func =: the log-likelihood function embedded in the sampled object rbind.data =: bind the rows of each predictive sample into a rectangular array, or leave as a list. gen_pp_data &lt;- function (sampled, n, ll_func = sampled$ll_func, rbind.data = TRUE) { sampled_stage &lt;- length(sampled$samples$stage[sampled$samples$stage == &quot;sample&quot;]) iterations &lt;- round(seq(from = (sampled$samples$idx - sampled_stage), to = sampled$samples$idx, length.out = n)) data &lt;- sampled$data S &lt;- sampled$n_subjects pp_data &lt;- list() for (s in 1:S){ print(paste0(&quot;subject&quot;, s)) for (i in 1:length(iterations)) { print(i) x &lt;- sampled$samples$alpha[, s, iterations[i]] names(x) &lt;- pars out &lt;- ll_func(x = x, data = data[data$subject == s, ], sample = TRUE) if (i == 1){ pp_data[[s]] = cbind(pp_subj = i, out) } else { pp_data[[s]] = rbind(pp_data[[s]], cbind(pp_subj = i, out)) } } } if (rbind.data){ tidy_pp_data &lt;- do.call(rbind, pp_data) return(tidy_pp_data) } else { return(pp_data) } } We generate 20 posterior predictive data samples. pp_data_3b &lt;- gen_pp_data(sampled, n = 20) The returned data is a matrix with the same dimensions and names as forstmann – with the addition of pp_iter column. pp_iter is the iteration of posterior sample (in this example i = 1:20) for the corresponding subject. We now have two matrices based on samples from either model. The response (resp) and response time (rt) columns now contain posterior predictive data. In the next section, we will use the posterior predictive data to assess descriptive adequacy. 3.5.1 Assessing Descriptive Adequacy (goodness of fit) Now we will plot the posterior predictive data against the real data. In the section below we compare observed RTs against predicted RTs, which is common for RT modelling; however, the code could also be modified for different types of data. # Subject x condition Q25, median and Q75 respone time + mean accuracy # Forstmann dataset pq3b &lt;- forstmann %&gt;% group_by(condition, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Subject x condition Q25, median and Q75 respone time for posterior predictive data pp_pq3b &lt;- pp_data_3b %&gt;% group_by(condition, pp_iter, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Combine data with posterior predictive data and add data source pq3b &lt;- bind_rows(cbind(src = rep(&quot;data&quot;, nrow(pq3b)), pq3b), cbind(src = rep(&quot;model&quot;, nrow(pp_pq3b)), pp_pq3b) ) # Mean Q25, median, Q4 and accuracy for data and posterior predictive data av_pq3b &lt;- pq3b %&gt;% group_by(src, condition) %&gt;% summarise_at(vars(Q25:acc), mean) # Variances of posterior samples pp_var3b &lt;- pq3b %&gt;% filter(src != &quot;data&quot;) %&gt;% group_by(condition, pp_iter, src) %&gt;% summarise_at(vars(Q25:acc), mean) # Convert source column to a factor and add labels av_pq3b$src &lt;- factor(av_pq3b$src, levels = c(&quot;model&quot;, &quot;data&quot;), labels = c(&quot;model&quot;, &quot;data&quot;) ) pp_varsrc3b &lt;- factor(pp_var3b$src, levels = c(&quot;model&quot;, &quot;data&quot;), labels = c(&quot;model&quot;, &quot;data&quot;) ) # Rename conditions levels(av_pq3b$condition) &lt;- levels(pp_var3b$condition) &lt;- c(&quot;Accuracy&quot;, &quot;Neutral&quot;, &quot;Speed&quot;) # Convert rt to milliseconds and acc to percentage av_pq3b$acc &lt;- 100 * av_pq3b$acc pp_var3b$acc &lt;- 100 * pp_var3b$acc av_pq3b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * av_pq3b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] pp_var3b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * pp_var3b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] 3.6 Evaluating different models - single threshold LBA Now let’s run through an example where the model differs and we assume that participants’s level of caution does not change with emphasis instructions i.e. the threshold for Accuracy, Neutral, and Speed conditions is the same. We will call this our “single threshold LBA model”. The single threhold LBA model’s log-likelihood has one b parameter, instead of three (see line 18, 31 and 64 below). For brevity, we specify only the ‘fast’ log-likelihood function for the single threshold model. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 fast_lba_ll1b &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } if (sample) { tmp &lt;- numeric(nrow(data)) data$rt &lt;- rep(NA, nrow(data)) data$resp &lt;- rep(NA, nrow(data)) } else { out &lt;- numeric(nrow(data)) } if (sample) { for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b = x[&quot;b&quot;] + A vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] t0 = x[&quot;t0&quot;] s = c(1, 1) if (data$stim[i] == 1) { vs = c(vc, ve) } else { vs = c(ve, vc) } tmp &lt;- rLBA(n = 1, A = A, b = b, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } } else { vlist = list(&quot;v.1&quot; = numeric(nrow(data)), &quot;v.2&quot; = numeric(nrow(data))) for (c in levels(data$condition)) { for (s in levels(data$stim)) { use &lt;- data$condition == c &amp; data$stim == s vc = x[&quot;vc&quot;] ve = x[&quot;ve&quot;] if (s == 1) { vlist$v.1[use] = vc vlist$v.2[use] = ve } else { vlist$v.1[use] = ve vlist$v.2[use] = vc } } } out &lt;- dLBA(rt = data$rt, response = data$resp, A = x[&quot;A&quot;], b = x[&quot;b&quot;] + x[&quot;A&quot;], mean_v = vlist, sd_v = c(1, 1), t0 = x[&quot;t0&quot;], distribution = &quot;norm&quot;, silent = TRUE) } if (sample) { return(data) } else { out&lt;-sum(log(pmax(out, 1e-10))) return(out) } } 3.6.1 PMwG framework for a single threshold model The PMwG sampler procedure remains the same for all models. For our three threshold LBA model, we need the updated log-likelihood function from above, an updated parameter vector, start points and priors. # Load the log-likelihood script source(file = &quot;fast_lba_ll1b.R&quot;) # Specify the parameter vector with single threshold (b) parameter pars &lt;- c(&quot;A&quot;,&quot;b&quot;,&quot;vc&quot;,&quot;ve&quot;,&quot;t0&quot;) # Specifiy a priors list priors &lt;- list( theta_mu = rep(0, length(pars)), theta_sig = diag(rep(1, length(pars))) ) # Setup your sampler object - include your data, parameter vector, # priors and log-likelihood function # Note: Your log-likelihood function must be loaded before this point sampler &lt;- pmwgs( data = forstmann, pars = pars, prior = priors, ll_func = fast_lba_ll1b ) # Start points are not included in this example # Initiatlise the sampler sampler &lt;- init(sampler) # Burn-in stage burned &lt;- run_stage(sampler, stage = &quot;burn&quot;, iter = 500, particles = 1000, epsilon = .5) # Adaptation stage adapted &lt;- run_stage(burned, stage = &quot;adapt&quot;, iter = 10000, particles = 1000, epsilon = .5) # Sample stage sampled &lt;- run_stage(adapted, stage = &quot;sample&quot;, iter = 1000, particles = 200, epsilon = .5) Note: we keep the priors, start points, number of iterations and particles the same as our three threshold LBA model, so there is no bias for either model. 3.7 Checking Descriptive Adequacy of 1b model. The checking procedure below is the same as the three threshold LBA model, except we use the single threshold LBA model’s sampled object/data. load(&quot;forstmann1b_sampled.RData&quot;) As we did for the three threshold LBA model, we generate 20 posterior predictive data samples. Note: this function is from section 3.5 pp_data_1b &lt;- gen_pp_data(sampled, n = 20) # Subject x condition Q25, median and Q75 respone time + mean accuracy # Forstmann dataset pq1b &lt;- forstmann %&gt;% group_by(condition, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Subject x condition Q25, median and Q75 respone time for posterior predictive data pp_pq1b &lt;- pp_data_1b %&gt;% group_by(condition, pp_iter, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Combine data with posterior predictive data and add data source pq1b &lt;- bind_rows(cbind(src = rep(&quot;data&quot;, nrow(pq1b)), pq1b), cbind(src = rep(&quot;model&quot;, nrow(pp_pq1b)), pp_pq1b) ) # Mean Q25, median, Q4 and accuracy for data and posterior predictive data av_pq1b &lt;- pq1b %&gt;% group_by(src, condition) %&gt;% summarise_at(vars(Q25:acc), mean) # Variances of posterior samples pp_var1b &lt;- pq1b %&gt;% filter(src != &quot;data&quot;) %&gt;% group_by(condition, pp_iter, src) %&gt;% summarise_at(vars(Q25:acc), mean) # Convert source column to a factor and add labels av_pq1b$src &lt;- factor(av_pq1b$src, levels = c(&quot;model&quot;, &quot;data&quot;), labels = c(&quot;model&quot;, &quot;data&quot;) ) pp_varsrc1b &lt;- factor(pp_var1b$src, levels = c(&quot;model&quot;, &quot;data&quot;), labels = c(&quot;model&quot;, &quot;data&quot;) ) # Rename conditions levels(av_pq1b$condition) &lt;- levels(pp_var1b$condition) &lt;- c(&quot;Accuracy&quot;, &quot;Neutral&quot;, &quot;Speed&quot;) # Convert rt to milliseconds and acc to percentage av_pq1b$acc &lt;- 100 * av_pq1b$acc pp_var1b$acc &lt;- 100 * pp_var1b$acc av_pq1b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * av_pq1b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] pp_var1b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * pp_var1b[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] 3.8 Model Comparison In this section, we are going to compare the three threshold model and the single threshold model for the forstmann (2008) data to determine which model best represents the data. We begin by using a graphical method i.e. plotting the modelled data against the observed data. 3.8.1 Assessing Descriptive Adequacy Graphically We can assess descriptive adequacy graphically by plotting the observed data, three threshold and single threshold data on a single plot. # Subject x condition Q25, median and Q75 respone time + mean accuracy # Forstmann data pq &lt;- forstmann %&gt;% group_by(condition, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Subject x condition Q25, median and Q75 respone time for posterior predictive data #3b model pp_pq3b &lt;- pp_data_3b %&gt;% group_by(condition, pp_iter, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Subject x condition Q25, median and Q75 respone time for posterior predictive data #1b model pp_pq1b &lt;- pp_data_1b %&gt;% group_by(condition, pp_iter, subject) %&gt;% summarise(Q25 = quantile(rt, prob = 0.25), median = median(rt), Q75 = quantile(rt, prob = 0.75), acc = mean(ifelse(stim == resp, 1, 0)), .groups = &quot;keep&quot; ) # Combine data with posterior predictive data for 1b and 3b models + add data source pqall &lt;- bind_rows(cbind(src = rep(&quot;data&quot;, nrow(pq)), pq), cbind(src = rep(&quot;3b&quot;, nrow(pp_pq3b)), pp_pq3b), cbind(src = rep(&quot;1b&quot;, nrow(pp_pq1b)), pp_pq1b) ) # Mean Q25, median, Q4 and accuracy for data and posterior predictive data av_pq &lt;- pqall %&gt;% group_by(src, condition) %&gt;% summarise_at(vars(Q25:acc), mean) # Variances of posterior samples pp_var &lt;- pqall %&gt;% filter(src != &quot;data&quot;) %&gt;% group_by(condition, pp_iter, src) %&gt;% summarise_at(vars(Q25:acc), mean) # Convert source column to a factor and add labels av_pq$src &lt;- factor(av_pq$src, levels = c(&quot;3b&quot;, &quot;data&quot;, &quot;1b&quot;), labels = c(&quot;3b&quot;, &quot;data&quot;, &quot;1b&quot;) ) pp_varsrc &lt;- factor(pp_var$src, levels = c(&quot;3b&quot;, &quot;data&quot;, &quot;1b&quot;), labels = c(&quot;3b&quot;, &quot;data&quot;, &quot;1b&quot;) ) # Rename conditions levels(av_pq$condition) &lt;- levels(pp_var$condition) &lt;- c(&quot;Accuracy&quot;, &quot;Neutral&quot;, &quot;Speed&quot;) # Convert rt to milliseconds and acc to percentage av_pq$acc &lt;- 100 * av_pq$acc pp_var$acc &lt;- 100 * pp_var$acc av_pq[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * av_pq[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] pp_var[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] &lt;- 1000 * pp_var[, c(&quot;Q25&quot;, &quot;median&quot;, &quot;Q75&quot;)] The plots show that values in the three threshold model (red circles) better describe the data than the single threshold model (blue circles). 3.8.2 Model comparison via DIC Another method of model comparison is using information criterions. Here we provide a function for calculating deviance information criterion (DIC). Note: we recommend using marginal likelihood (Bayes factors) instead of DIC for model selection. For more information, see this paper on estimating the Marginal Likelihood via importance sampling. pmwg_DIC &lt;- function(sampled, pD = FALSE){ # Identify number of subjects nsubj &lt;- length(unique(sampled$data$subject)) # Mean log-likelihood of the overall (sampled-stage) model, for each subject mean_ll &lt;- apply(sampled$samples$subj_ll[, sampled$samples$stage == &quot;sample&quot;], 1, mean) # Mean of each parameter across iterations. # Keep dimensions for parameters and subjects mean_pars &lt;- t(apply(sampled$samples$alpha, 1:2, mean)) # Name &#39;mean_pars&#39; so it can be used by the log_like function colnames(mean_pars) &lt;- sampled$par_names # log-likelihood for each subject using their mean parameter vector mean_pars_ll &lt;- numeric(ncol(mean_pars)) data &lt;- transform(sampled$data, subject = match(subject, unique(subject))) for (j in 1:nsubj) { mean_pars_ll[j] &lt;- sampled$ll_func(mean_pars[j, ], data = data[data$subject == j,], sample = FALSE) } # Effective number of parameters pD &lt;- sum(-2 * mean_ll + 2 * mean_pars_ll) # Deviance Information Criterion DIC &lt;- sum(-4 * mean_ll + 2 * mean_pars_ll) if (pD){ return(c(&quot;DIC &quot; = DIC, &quot; Effective parameters&quot; = pD)) }else{ return(DIC) } } We calculate the DIC value for each model by passing the sampled object into the pmwgDIC function. load(&quot;forstmann3b_sampled.RData&quot;) sampled3b &lt;- sampled pmwg_DIC(sampled = sampled3b) DIC Effective parameters -15431.90924 41.38816 load(&quot;forstmann1b_sampled.RData&quot;) sampled1b &lt;- sampled pmwg_DIC(sampled = sampled1b) DIC Effective parameters -10818.301739 5.489469 The three threshold model has a lower DIC value than the single threshold model, indicating a better explanation of the data. Based on the graphical evidence and the DIC, we can be confident that the three threshold model (i.e the model where threshold is varied with speed and accuracy instructions) is a better fit to our data than the single threshold model. 3.9 Checking the LBA log-likelihood function Here we show two ways to test your log-likelihood function. This method is not fail-safe, however, it allows you to establish whether your function operates as intended. We will use the three threshold LBA model log-likelihood function in the following examples. 3.9.1 Test one: Do changes in parameter values cause changes in the returned log-likelihood? We will test our log-likelihood functions by passing a parameter vector (x) of plausible, arbitrary values, followed by a second parameter vector with different, plausible, arbitrary values. If the log-likehood functions are functioning correctly, we should see the log-likelihood value change with the change in parameter values. We may also wish to check that unrealistic parameter values are effectively dealt with (for example, t0 should not be able to be greater than any RTs). Let’s compare the output from the trialwise log-likelihood function in section 3.3 and the fast log-likelihood function in section 3.3.1 : x &lt;- log(c(A = 2, b.1 = 2, b.2 = 1, b.3 = .5, vc = 4, ve = 2, t0 = .18 ) ) tw_lba_ll(x, forstmann, sample = FALSE) ## [1] -44505.89 fast_lba_ll3b(x, forstmann, sample = FALSE) ## [1] -44505.89 Great – the log-likelihoods are the same. Now let’s change the parameter values, in particular, the three threshold parameters, and see if the log-likelihood changes. x &lt;- log(c(A = 2, b.1 = 1, b.2 = 3, b.3 = 3, vc = 4, ve = 2, t0 = .18 ) ) tw_lba_ll(x, forstmann, sample = FALSE) ## [1] -202667.3 fast_lba_ll3b(x, forstmann, sample = FALSE) ## [1] -202667.3 These log-likelihoods are lower than the previous two, so the parameter (x) values are less accurate given the data. Notice how we can also use this process to check the fast, efficient log-likelihood function (fast_lba_ll3b) against the trialwise function (tw_lba_ll). Given the same data and parameter values - they should give the same output, as we see above. Not: We only changed some named (x) parameter values. You could perform a thorough check by changing one parameter value at a time and see if there is a change in the log-likelihood. This is a good first check to see if our log-likelihood is behaving as it should; changing log-likelihood value with changing parameter values. However, our log-likelihood functions may contain inconsistencies, which leads us onto our second check. 3.9.2 Testing whether true-x values have the highest likelihood A more comprehensive way to test your log-likelihood function is to generate ‘synthetic’ data for which you know the true data generating values. This is a form of ‘parameter recovery’, where you run the sampler on generated data and plot the log-likelihood change as x values move away from your ‘true’ x-value. If the true-x has the highest log-likelihood, then it is likely that your function is working as intended. There are several steps to this method: Assign values to parameters of the model - these are known as the generating values. Note: ensure these are sensible values - for example in the LBA, we want the drift rate for the mismatch/error accumulator (ve) to be smaller than the match/correct accumulator (vc) and the non-decision time parameter t0 should not be too large. true_x &lt;- log(c(A = 2, b.1 = 2, b.2 = 1, b.3 = .5, vc = 4, ve = 2, t0 = .18 ) ) Pass the generating parameter values (true_x) into the log-likelihood and generate data using your log-likelihood function, where sample = TRUE. test_data &lt;- tw_lba_ll(x = true_x, data = data, sample = TRUE ) ## Warning in rnorm(length(ind.no)): &#39;.Random.seed[1]&#39; is not a valid integer, so ## ignored Generate profile plots to compare the log-likelihood of the true_x parameter values against nearby values in the ‘synethetic’ data. You can write your own function for this, or use the function below. Our plotting function takes five arguments: x is a vector of named parameter values. n_values specifies how many parameter x-values we will assess to compare to compare to the true data-generating parameter value. Default is 9. synth_data is the synthetic data dims specifies the layout for the plots we will generate. In this example, we have 7 parameters, so I will set my dimensions to be 4 rows with 2 columns. ll_func specifies the name of the log-likelihood function you have created. server should only be set to true if running on a device with multiple cores. This argument speeds up computational efficiency. library(parallel) test_ll &lt;- function(x, n_values = 9, synth_data, dims, ll_func, server = FALSE,...) { pars &lt;- names(x) xtmp &lt;- x sequence &lt;- seq(from = -.22, to = .22, length.out = n_values) op &lt;- par(mfrow = dims) par_likelihoods &lt;- lapply(setNames(pars, pars), function(p){ testvalues &lt;- sequence + true_x[[p]] tmp &lt;- unlist(lapply(testvalues, function (i){ #for each test vlaue, it will apply the function where i = testvalue. xtmp[[p]] &lt;- i ll_func(x = xtmp, data = synth_data) })) plot(x = testvalues, y = tmp, type = &quot;b&quot;, main = p, xlab = &quot;log par values&quot;, ylab = &quot;log-likelihood&quot;, ...) abline(v = true_x[[p]], col = &quot;red&quot;) return(tmp)}) par(op) return(par_likelihoods) } Whilst we used the trialwise log-likelihood function to generate data, we will use the fast log-likelihood function in this “parameter recovery” simulation for computational efficiency. ll_plots &lt;- test_ll(x = true_x, n_values = 9, synth_data = test_data, dims = c(2, 2), ll_func = fast_lba_ll3b ) The red line indicates the true generating parameter value (i.e. the data generating values for the parameters) have the best (highest) likelihoods. Note: The plots above do not tell you if your log-likelihood is correct in the sense that you have written a log-likelihood for the model you actually want to test. The plots allow you to determine if your function is functioning as intended. Below is output from a function with a potential error. We can see that the red lines indicating the generating parameter values (i.e. the data generating values for the parameters) do not have the best (highest) likelihoods. Plots with flat lines for parameter values would also be indicative of an error somewhere within your log-likelihood code. References "],
["pmwg-sampler-with-the-linear-ballistic-accumulator-and-a-complex-experiment-design.html", "Chapter 4 PMwG sampler with the Linear Ballistic Accumulator and a complex experiment design 4.1 Writing the LBA log-likelihood function for the Wagenmakers data set", " Chapter 4 PMwG sampler with the Linear Ballistic Accumulator and a complex experiment design In chapter 1 we demonstrated how the PMwG sampler can be used to model a lexical decision task in a signal detection framework. However, the SDT framework does not allow us to consider response time (RT) and the join distribution of RT and accuracy. In this example we will expand on what was covered in chapter 1, by fitting the LBA - a more complex model - to the Wagenmakers 2008 data. A description of the Wagenmakers experiment and data is covered in chapter 1. This experiment is more complicated than the Forstmann example in chapter 3, and the LBA is also more complicated than the SDT model. As a result, the log-likelihood function for this example will be more complex, however, you’ll notice that every step we take closely follows those taken in previous chapters with simpler data sets and simpler models. 4.1 Writing the LBA log-likelihood function for the Wagenmakers data set Here we’ve written a slow, but easy to follow log-likelihood function. The log-likelihood function steps through the data line by line (i.e. trial by trial) and gives a likelihood value for each line under x parameters. As mentioned in previous chapters, we encourage those who have experience writing log-likelihood functions to write a computationally efficient function or use our ’fastfunction “HERE”. For those new to modelling, the trialwise function directly below is easier to follow, debug and is less likely to result in errors. The structure of our log-likelihood function follows those in the preceding chapters, so we will only step through the parts that differ i.e. the experiment design and hypothesis about which parameters are being influenced by the experimental manipulations. Let’s begin by loading rtdists package… library(rtdists) and now a complete trialwise (slow) log-likelihood function. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 tw_lba_ll &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } if (sample) { data$rt &lt;- rep(NA, nrow(data)) data$resp &lt;- rep(NA, nrow(data)) } else { out &lt;- numeric(nrow(data)) } for (i in 1:nrow(data)) { A = x[&quot;A&quot;] b.w = x[paste0(&quot;b.&quot;, data$cond[i], &quot;.W&quot;)] + A b.nw = x[paste0(&quot;b.&quot;, data$cond[i], &quot;.NW&quot;)] + A bs = list(b.nw, b.w) v.w = x[paste0(&quot;v.&quot;, data$stim[i], &quot;.W&quot;)] v.nw = x[paste0(&quot;v.&quot;, data$stim[i], &quot;.NW&quot;)] vs = c(v.nw, v.w) t0 = x[&quot;t0&quot;] s = c(1, 1) if (sample) { tmp &lt;- rLBA(n = 1, A = A, b = bs, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) data$rt[i] &lt;- tmp$rt data$resp[i] &lt;- tmp$resp } else { out[i] &lt;- dLBA(rt = data$rt[i], response = data$resp[i], A = A, b = bs, mean_v = vs, sd_v = s, t0 = t0, dist = &quot;norm&quot;, silent = TRUE ) } } if (sample) { return(data) } else { bad &lt;- (out &lt; 1e-10) | (!is.finite(out)) out[bad] &lt;- 1e-10 out &lt;- sum(log(out)) return(out) } } We begin from the for loop on line 14. In this for loop, we assign the values (x) of each parameter in our model, for each row in the dataset, so that any conditional parameters (for example b in our model) are correctly assigned. For the Wagenmakers data set, we want to calculate the density function for a model that has a threshold (b) parameter for each of the conditions (W: 75% stimuli words, NW: 75% stimuli nonwords), but also allows threshold to vary with response (i.e., the accumulator for a word response, and the accumulator for the non-word response), we include lines 14 - 23. These lines take into account the two conditions in the proportion factor by pasting &quot;b.&quot;, the condition (W, NW) that is on line [i], and whether it is a threshold for the word accumulator &quot;.W&quot; or nonword accumulator &quot;.NW&quot;. Again, we add the sart point parameter A value to each threshold parameter so that threshold is greater than the start point value. We also hypothesised that drift rate would vary with word frequency (hf,lf,vlf,nw), so on lines 19 and 20 we allow drift rate for response word (v.w) to vary with the levels of word frequency, by pasting the &quot;v.&quot; with the word frequency on line [i], and with the accumulator (&quot;.W&quot; or &quot;.NW&quot;). You’ll notice that in this example, we no longer have a drift rate for the correct response (vc) or incorrect response (ve), instead, we have a drift rate for responding word (v.w) and non-word (v.nw). This is just a different way of coding drift rate, and is our preferred way. On line x we have coded the vs vector to have v.nw first and v.w second. "],
["troubleshoot.html", "Chapter 5 Troubleshooting PMwG errors 5.1 How to write a log-likelihood function", " Chapter 5 Troubleshooting PMwG errors 5.1 How to write a log-likelihood function What key elements are required in a log-likelihood function to be used in the sampler Show comparison times for slow LL and fast LL Check list for commmon errors - brief list check this, check that etc. 5.1.1 Writing your log-likelihood function: Tips, errors and check list 1. The parameter specified does not exist The parameter name is not specified to be estimated i.e. it is not in the parameter names argument or it is misspelled. Make sure pars vector contains the same parameter names you have included in your log-likelihood function and it is the same length. Do not rely on the log likelihood function to throw an error in this case. (e.g.x[‘b’]) 2. All non-continuous data frame variables must be a factor. Data frame variables should be factors unless the variable is a continuous variable e.g. response time. If you pass character variables to if statements and/or for loops in your log likelihood function, errors will not occur, however, your log likelihood estimate will be incorrect. For example, avoid using character strings like data$condition == “easy”. If you must use a character string, be sure to convert the string to a factor with as.factor. 3. Spelling errors or mismatched column name references Correctly reference data frame column names in your log likelihood function e.g. data$RT != data$rt. 4. When initialising a vector of parameter values - values are not filling in properly E.g. When a vector for b for all the values across the data set to be used, but there are NAs filling it somewhere. 5. Make sure operations are done on the right scale. 6. Data frame variables are scaled appropriately for the model Check your variables are correctly scaled and in the correct units. For example, with the LBA, response times must be in seconds rather than milliseconds. 7. The log-likelihood is printed/outputted at the end of function Make sure your log-likelihood function prints an estimate at the end of the function and the estimate is correctly obtained e.g. sum the log-likelihood estimates for all trials/rows. 8. Sampling error occurs When sampling, the generated columns are not outputted 9. When executing functions row by row (i.e. trial-wise), index MUST be included If writing a trial-wise/row-wise function (e.g. if statement, for loop), index i must be specified. if (data$condition == “easy”) # Incorrect reference when iterating over variable if (data$condition[i] == “easy”) # Include i index 10. Changing parameter values changes the log-likelihood estimate A simple check to run on your log-likelihood function is to modify your parameter values and observe the change to log-likelihood estimate. Then check if changing parameter values which rely on conditions actually change the log-likelihood estimate. 11. Make sure you have the latest version of the PMwG Samplers package checkVersion(“pmwg”) 12. Number of iterations for ’burn-in` stage of sampler. We suggest running burn-in for few iterations and particles first. This will give you a sense of a) whether the sampler is working as intended (see troubleshooting/checks for what parameter chains should look like), b) the number of iterations &amp; particles needed to achieve the target acceptance rate, as well as the appropriate epsilon value. The acceptance rate is generally very high for the first few iterations ( &gt; 100) and then declines. After the initial short run, you can check and optimise the number of particles to be used (and balance with epsilon), so the acceptance rate is close to 30% on a longer, full run. We recommend you start with epsilon = .5 to increase efficiency, then adjust as needed. NOTE: Overall, we aim for ~30% acceptance rate of particles. High acceptance rates may be inaccurate if burn-in runs for few iterations. Low acceptance rates are inefficient and may fail to create an efficient distribution for the sampling stage. "],
["appendix.html", "Chapter 6 Appendix 6.1 Wagenmakers SDT script", " Chapter 6 Appendix 6.1 Wagenmakers SDT script Use the script below to process the data file so that the dataset structure is the same as shown in our SDT example. A zipped folder containing the dataset from Wagenmakers et al. (2008) can be found at this link. As shown in our script below, the data is stored in the &quot;PropData.txt&quot; file. #~~~~~~~~~~~~~~~~~~~~ Wagenmakers 2008 Lexical decision task ~~~~~~~~~~~~~~~~~~~~~~~~~~~# # 1) subject = participant number # 2) block = block number # 3) practice = 1 if practice block, otherwise 0 # 4) cond = condition either &quot;2&quot; for 75% words or &quot;1&quot; for 75% nonwords) # 5) stimulus = unique identifier of stimulus, stimuli are nested in frequency conditions # 6) freq = Code &quot;1&quot; means &quot;high frequency word&quot;, code &quot;2&quot; means &quot;low frequency word&quot;, # and code &quot;3&quot; means &quot;very low frequency word&quot;. Codes 4, 5, and 6 = &quot;nonword&quot;. # 7) resp = 0 is nonword, 1 is word, -1 is not interpretable response (i.e., pushed a button, # but not the right one and also not the one next to the right button) # 8) rt = response time in seconds # 9) censor = 1 if value is eliminated from further analysis; # practice block, uninterpretable response, too fast response (&lt;180 ms), too slow response (&gt;3 sec) rm(list=ls()) require(tidyverse) wagenmakers2008 &lt;- read.delim(&quot;PropData.txt&quot;, header = FALSE) names(wagenmakers2008) &lt;- c(&quot;subject&quot;,&quot;block&quot;,&quot;practice&quot;, &quot;cond&quot;,&quot;stimulus&quot;,&quot;freq&quot;,&quot;resp&quot;,&quot;rt&quot;, &quot;censor&quot;) wagenmakers2008 &lt;- wagenmakers2008[wagenmakers2008$censor!=1,-c(3,9)] wagenmakers2008$subject &lt;- factor(wagenmakers2008$subject) wagenmakers2008$cond &lt;- factor(wagenmakers2008$cond, labels = c(&quot;nw&quot;,&quot;w&quot;)) wagenmakers2008$stimulus &lt;- wagenmakers2008$freq &lt; 4 wagenmakers2008$stimulus &lt;- factor(wagenmakers2008$stimulus, labels = c(&quot;nw&quot;, &quot;w&quot;)) wagenmakers2008$resp &lt;- factor(wagenmakers2008$resp, labels = c(&quot;NW&quot;,&quot;W&quot;)) wagenmakers2008$freq &lt;- ((wagenmakers2008$freq-1) %% 3)+1 wagenmakers2008$freq &lt;- factor(wagenmakers2008$freq, labels = c(&quot;hf&quot;,&quot;lf&quot;,&quot;vlf&quot;)) wagenmakers2008$correct &lt;- toupper(wagenmakers2008$stimulus) == toupper(wagenmakers2008$resp) wagenmakers2008$W &lt;- as.character(wagenmakers2008$freq) wagenmakers2008$W[wagenmakers2008$stimulus == &quot;nw&quot;] &lt;- &quot;nw&quot; wagenmakers2008$W &lt;- factor(wagenmakers2008$W, c(&quot;hf&quot;,&quot;lf&quot;,&quot;vlf&quot;,&quot;nw&quot;)) wagenmakers2008 &lt;- select(.data = wagenmakers2008, subject, cond, W, resp, rt, correct) %&gt;% rename(stimulus = W) wagenmakers2008$correct &lt;- if_else(wagenmakers2008$correct, true = &quot;2&quot;, false = &quot;1&quot;) save(wagenmakers2008, file = &quot;wagenmakers2008.RData&quot;) References "],
["references.html", "References", " References "]
]
