[
["index.html", "Particle Based Samplers for MCMC Chapter 1 Introduction to Particle Based Sampler for MCMC 1.1 Assumed knowledge 1.2 Computational Requirements 1.3 Background information", " Particle Based Samplers for MCMC Jon-Paul Cavallaro Wednesday 01 April 2020 Chapter 1 Introduction to Particle Based Sampler for MCMC Contains implementations of particle based sampling methods for model parameter estimation. Primarily an implementation of the Particle Metropolis within Gibbs sampler outlined in the paper available at https://arxiv.org/abs/1806.10089, it also contains code for covariance estimation and time varying models. 1.1 Assumed knowledge Modelling knowledge Familiarity with particular packages? E.g. rtdists 1.2 Computational Requirements R Version Packages Memory Document computational requirements - memory 200mb Minutes Size of samples Memory required Can it be done on a grid or laptop 1.3 Background information What is particle metropolis What is Gibbs? Why a multivariate normal? The prior is ‘fixed’ "],
["sampler-application-example-1-forstmann-et-al-2008-dataset.html", "Chapter 2 Sampler Application Example 1 - Forstmann et al. (2008) dataset 2.1 Description of Forstmann experiment 2.2 Setting up the sampler", " Chapter 2 Sampler Application Example 1 - Forstmann et al. (2008) dataset We begin with a simple use case of the psamplers package with data from Forstmann et al. (2008). 2.1 Description of Forstmann experiment First we need to install the psamplers package. We currently recommended installing psamplers via devtools. # The samplers package will be on CRAN - this step will be removed. install_github(&#39;gjcooper\\samplers&#39;) library(psamplers) Forstmann et al looked at neural correlates of decision making under time pressure, with an aim to identify areas of the brain associated with speed-accuracy tradeoff. Imaging (fMRI) and behavioural data was collected; however, we will analyse behavioural data from the decision-making task only. In terms of modelling the data, Forstmann expected to find differences in thresholds (direction?) for each of the three speed-emphasis conditions. We have included the Forstmann et als data in the psamplers package as a data frame object named forstmann. The sampler requires a data frame with a subject column. The subject column data type can be a factor or numeric. Table 2.1 shows the first ten trials from the Forstmann dataset. Participants (n = 19) were asked to indicate whether a cloud of dots in a random-dot kinematogram (RDK) moved to the left or the right of the screen. The IV was a within-subject, speed-accuracy manipulation where, before each trial began, pariticipants were instructed to make their choice accurately (condition = 1), with urgency(condition = 3)or were presented with a neutral message (condition = 2). Choice and response time data was collected. Choices were coded as correct (correct = 2) or incorrect (correct = 1) and response times (rt) were recorded in seconds. For more information about the design of the experiment please see the original paper. Table 2.1: First 10 trials in Forstmann dataset. The forstmann dataset is an object/data frame subject rt correct condition 1 0.4319 2 1 1 0.5015 2 3 1 0.3104 2 3 1 0.4809 2 1 1 0.3415 2 1 1 0.3465 2 2 1 0.3572 2 2 1 0.4042 2 2 1 0.3866 2 2 1 0.3683 2 1 2.2 Setting up the sampler We will use the Linear Ballistic Accumulator (LBA) (Brown and Heathcote 2008) to demonstrate how to use the psamplers package. The LBA model parameters are: b threshold parameter (the evidence required to make a response) v is drift rate or average speed of evidence accumulation A is the model’s start point t0 is non-decision time. Do we need to mention sv here? We begin by creating a vector of parameter names for the model. You can name this object as you wish; however, in our example, we will call it pars. The parameters you list in the pars vector must match the names and number of parameters you include in your likelihood function. pars &lt;- c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;, &quot;A&quot;, &quot;v1&quot;, &quot;v2&quot;, &quot;t0&quot;) For this dataset, we use three threshold parameters (b1, b2, and b3 i.e. one for each condition) because we assume that the participant responds to each condition with a different level of caution. We include two drift rate parameters: v1 for the incorrect accumulator and v2 for the correct accumulator, a start point parameter A and a non-decision time t0 parameter. We’ve made a decision to set the sv to 1 to satisfy the scaling properties of the model, as such we haven’t included the sv parameter in the pars vector - it is found in the LBA’s likelihood function (see below). Next we create a priors object; a list that contains two components theta_mu a vector containing the prior for model parameter means theta_sig the prior covariance matrix for model parameters. priors &lt;- list(theta_mu = rep(0, length(pars)), theta_sig = diag(rep(1, length(pars))) ) The priors object in our example is initiated with zeros. Under what conditions would this priors object differ? The next step is to include your log likelihood function. This must be called before you create the sampler object in the following step. You can load your log likelihood function from an external script: source(file = &quot;yourLogLikelihoodFile.R&quot;) Or you can write a log likelihood function as we have done with the LBA log likelihood function below. If you’d like to run through this example, it is best to copy the lba_loglike function from the code block below rather than copying from the following separate code chunks, as some curly braces have been removed from code chunks. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 lba_loglike &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } bs &lt;- x[&quot;A&quot;] + x[c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;)][data$condition] if (sample) { out &lt;- rtdists::rLBA(n = nrow(data), A = x[&quot;A&quot;], b = bs, t0 = x[&quot;t0&quot;], mean_v = x[c(&quot;v1&quot;, &quot;v2&quot;)], sd_v = c(1, 1), distribution = &quot;norm&quot;, silent = TRUE) } else { out &lt;- rtdists::dLBA(rt = data$rt, response = data$correct, A = x[&quot;A&quot;], b = bs, t0 = x[&quot;t0&quot;], mean_v = x[c(&quot;v1&quot;, &quot;v2&quot;)], sd_v = c(1, 1), distribution = &quot;norm&quot;, silent = TRUE) bad &lt;- (out &lt; 1e-10) | (!is.finite(out)) out[bad] &lt;- 1e-10 out &lt;- sum(log(out)) } out } The lba_loglike function takes three arguments: x is a named parameter vector (e.g. pars) data is your data set (e.g.forstmann) sample = default is FALSE calculates a density function or set to TRUE to generate a sample that matches the shape of data. The first line in the lba_loglike function (Line 2 below) takes the exponent of the parameter values to move all parameter values to the real line. The purpose of this is to….. Line 3 and 4 then checks RTs are faster than the non-decision time parameter, and zero those RTs that are faster than non-decision time. 1 2 3 4 5 lba_loglike &lt;- function(x, data, sample = FALSE) { x &lt;- exp(x) if (any(data$rt &lt; x[&quot;t0&quot;])) { return(-1e10) } Next (Line 7) we create a vector containing threshold parameters for each row in the data set that takes into account the condition and adds the start point value. We add the start point parameter A value to each threshold parameter so that threshold is greater than the start point value. 7 bs &lt;- x[&quot;A&quot;] + x[c(&quot;b1&quot;, &quot;b2&quot;, &quot;b3&quot;)][data$condition] The if else statement below (Line 9-32) does one of two things: if (sample) calculates the posterior predictive data and the else statement calculates the density function. Toward the end of the else statment (line 28) we take all implausible likelihood values, assign them to the bad object and set them to zero. The final line in the else statement takes the log of all likelihood values, sums them and then assigns the model’s log likelihood value to the out variable. 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 if (sample) { out &lt;- rtdists::rLBA(n = nrow(data), A = x[&quot;A&quot;], b = bs, t0 = x[&quot;t0&quot;], mean_v = x[c(&quot;v1&quot;, &quot;v2&quot;)], sd_v = c(1, 1), distribution = &quot;norm&quot;, silent = TRUE) } else { out &lt;- rtdists::dLBA(rt = data$rt, response = data$correct, A = x[&quot;A&quot;], b = bs, t0 = x[&quot;t0&quot;], mean_v = x[c(&quot;v1&quot;, &quot;v2&quot;)], sd_v = c(1, 1), distribution = &quot;norm&quot;, silent = TRUE) bad &lt;- (out &lt; 1e-10) | (!is.finite(out)) out[bad] &lt;- 1e-10 out &lt;- sum(log(out)) } out Once you’ve setup your parameters, priors and your log likelihood function, the next step is to initialise the sampler object. sampler &lt;- pmwgs( data = forstmann, pars = pars, prior = priors, ll_func = lba_loglike ) The pmwgs function takes a set of arguments (listed below) and returns a list containing the required components for performing the particle metropolis within Gibbs steps. data = your data - a data frame (e.g.forstmann) with a column for participants called subject pars = the model parameters to be used (e.g.pars) prior = the priors to be used (e.g.priors) ll_func = name of log likelihood function you’ve sourced above (e.g.lba_loglike) 2.2.1 Model start points You have the option to set model start points. We have specified sensible start points for the Forstmann dataset. If you chose not to specify start points, the sampler will randomly sample points from the prior distribution. start_points &lt;- list( mu = c(.2, .2, .2, .4, .3, 1.3, -2), sig2 = diag(rep(.01, length(pars))) ) The start_points object contains two vectors: mu a vector of start points for the mu of each model parameter sig2 vector containing the start points of the covariance matrix of covariance between model parameters. 2.2.2 Running the sampler Okay - now we are ready to run the sampler. sampler &lt;- init(sampler, theta_mu = start_points$mu, theta_sig = start_points$sig2) Here we are using the init function to generate initial start points for the random effects and storing them in the sampler object. First we pass the sampler object from above that includes our data, parameters, priors and log likelihood function. If we decided to specify our own start points (as above), we would include the theta_mu and theta_sig arguments. Now we can run the sampler using the run_stage function. The run_stage function takes four arguments: x the sampler object including parameters stage = the sampling stage (e.g. &quot;burn&quot;, &quot;adapt&quot; or &quot;sample&quot;) iter = is the number of iterations for the sampling stage particles = is the number of particles generated on each iteration It is optional to include the iter = and particles = arguments. If these are not included, iter and particles default to 1000. The number of iterations you choose for your burn in stage is similar to choices made when running deMCMC, however, this varies depending on the time the model takes to reach the ‘real’ posterior space. First we run our burn in stage by setting stage = to &quot;burn&quot;. Here we have set iterations to be 500, which may take some time. burned &lt;- run_stage(sampler, stage = &quot;burn&quot;, iter = 500, particles = 1000) Now we run our adaptation stage by setting stage = &quot;adapt&quot; Because we have not specified number of iterations or particles, the sampler will use the default value of 1000 for each of these arguments. N.B. The sampler will quit adaptation stage after 20 unique values have been accepted for each subject. This means adaptation may not use all 1000 iterations. adapted &lt;- run_stage(burned, stage = &quot;adapt&quot;) At the start of the sampled stage, the sampler object will create a ‘proposal’ distribution for each subject’s random effects using a conditional multi-variate normal. This proposal distribution is then used to efficiently generate new particles for each subject which means we can reduce the number of particles on each iteration whilst still achieving acceptance rates. sampled &lt;- run_stage(adapted, stage = &quot;sample&quot;, iter = 100, particles = 100) References "],
["sampler-application-example-2-single-threshold-parameter.html", "Chapter 3 Sampler Application Example 2 - Single threshold parameter", " Chapter 3 Sampler Application Example 2 - Single threshold parameter In this second example, we will use a single b threshold parameter (the evidence required to make a response). Altering the code/model is simple. First, update your parameter object (e.g.pars) by including a single b threshold parameter: pars &lt;- c(&quot;b&quot;, &quot;A&quot;, &quot;v1&quot;, &quot;v2&quot;, &quot;t0&quot;) The priors object will update based on the length of your parameter object. Remove this? priors &lt;- list(theta_mu = rep(0, length(pars)), theta_sig = diag(rep(1, length(pars))) ) The next step is to modify your log likelihood function by updating the threshold parameter vector (bs) so that the name and number of threshold parameters (b) matches the name and number of threshold parameters you’ve specified in your pars vector. In this case, we require a single b parameter. bs &lt;- x[&quot;A&quot;] + x[&quot;b&quot;][data$condition] The subsequent lines of code in the log likelihood function remain unchanged. As mentioned in example 1, you may set model start points. It is important to provide the same number of values for mu as the number of parameters you’ve set in your pars vector. start_points &lt;- list( mu = c(.2,.4, .3, 1.3, -2), # We have set five parameter start points here, # matching the number of parameters in the pars vector. sig2 = diag(rep(.01, length(pars))) ) And now you’re ready to run the sampler as outlined in example 1. "],
["sampler-application-example-3-wagenmakers-2008-experiment-2.html", "Chapter 4 Sampler Application Example 3 - Wagenmakers (2008) Experiment 2", " Chapter 4 Sampler Application Example 3 - Wagenmakers (2008) Experiment 2 See iSquared paper eperiment 2 "],
["common-problems-better-name-required-troubleshooting-page.html", "Chapter 5 Common Problems (Better name required) Troubleshooting page 5.1 How to write a log likelihood function", " Chapter 5 Common Problems (Better name required) Troubleshooting page 5.1 How to write a log likelihood function What key elements are required in a log likelihood function to be used in the sampler Show comparison times for slow LL and fast LL Check list for commmon errors - brief list check this, check that etc. 5.1.1 Writing your log likelihood function: Tips, errors and check list 1. The parameter specified does not exist The parameter name is not specified to be estimated i.e. it is not in the parameter names argument or it is misspelled. Make sure pars vector contains the same parameter names you have included in your log likelihood function and it is the same length. Do not rely on the log likelihood function to throw an error in this case. (e.g.x[‘b’]) 2. All non-continuous data frame variables must be a factor. Data frame variables should be factors unless the variable is a continuous variable e.g. response time. If you pass character variables to if statements and/or for loops in your log likelihood function, errors will not occur, however, your log likelihood estimate will be incorrect. For example, avoid using character strings like data$condition == “easy”. If you must use a character string, be sure to convert the string to a factor with as.factor. 3. Spelling errors or mismatched column name references Correctly reference data frame column names in your log likelihood function e.g. data$RT != data$rt. 4. When initialising a vector of parameter values - values are not filling in properly E.g. When a vector for b for all the values across the data set to be used, but there are NAs filling it somewhere. 5. Make sure operations are done on the right scale. 6. Data frame variables are scaled appropriately for the model Check your variables are correctly scaled and in the correct units. For example, with the LBA, response times must be in seconds rather than milliseconds. 7. The log likelihood is printed/outputted at the end of function Make sure your log likelihood function prints an estimate at the end of the function and the estimate is correctly obtained e.g. sum the log likelihood estimates for all trials/rows. 8. Sampling error occurs When sampling, the generated columns are not outputted 9. When executing functions row by row (i.e. trial-wise), index MUST be included If writing a trial-wise/row-wise function (e.g. if statement, for loop), index i must be specified. if (data$condition == “easy”) # Incorrect reference when iterating over variable if (data$condition[i] == “easy”) # Include i index 10. Changing parameter values changes the log likelihood estimate A simple check to run on your log likelihood function is to modify your parameter values and observe the change to log likelihood estimate. Then check if changing parameter values which rely on conditions actually change the log likelihood estimate. "],
["how-to-use-the-pmwg-sampler-signal-detection-theory.html", "Chapter 6 How to use the PMwG sampler - Signal Detection Theory 6.1 Description of Wagenmakers experiment", " Chapter 6 How to use the PMwG sampler - Signal Detection Theory Here we demonstrate how to use the PMwG sampler package to run a simple signal detection theory (SDT) analysis on a lexical decision task taken from Wagenmakers et al. (2008). We recognise that it is unnecessary to use the sampler package for a simple analysis such as this; however, we hope this example demonstrates the usefulness of the samplers package. Stolen from Wagenmakers paper “This is analogous to a signal detection analysis that allows one to disentangle effects of stimulus discriminability (e.g., d0) from those of criterion placement (i.e., b).” 6.1 Description of Wagenmakers experiment Participants were asked to indicate whether a letter string was a word or a non-word. A subset of Wagenmaker et al data are shown in table 6.1. We have a subject column with a subject id (1-19), a condition column cond which indicates the proportion of words to non-words presented within a block of trials. In word blocks (cond = w) participants completed 75% word and 25% non-word trials and for non-word (cond = nw) blocks 75% non-word and 25% word trials. The stim column lists the word’s frequency i.e. is the stimulus a very low frequency word (cond = vlf), a low frequency word (cond = lf) or a high frequency word (cond = hf). The third column resp refers to the participant’s response i.e. the participant responded word (resp = W) or non-word (resp = NW). The two remaining columns list the response time (rt) and whether the paricipant made a correct (correct = 2) or incorrect (correct = 1) choice. For more details about the experiment please see the original paper. Is this the correct paper to reference? Table 6.1: Subset of 10 trials from the Wagenmakers (2008) dataset. subject prop freq resp rt correct 1 w lf W 0.410 2 1 w hf W 0.426 2 1 w nw NW 0.499 2 1 w lf W 0.392 2 1 w vlf W 0.435 2 1 w hf W 0.376 2 1 nw lf W 0.861 2 1 nw hf W 0.563 2 1 nw nw NW 0.666 2 1 nw nw NW 1.561 2 1 nw nw NW 0.503 2 1 nw nw NW 0.445 2 6.1.1 Signal Detection Theory We assume you have an understanding of SDT so we’ll jump to an explanation of how we can use SDT in the context of the lexical decision task. We begin with the distributions for non-word and word stimuli. You can think of these two distributions as the ‘noise’ and ‘signal’ curves, respectively. Each distribution represents the evidence for ‘word-likeness’ and they are assumed to be normally distributed. The non-word distribution (or the ‘noise’ distribution) has a mean of 0 and a standard deviation (SD) of 1 (Note: we could estimate SD here, but we will use 1 in this example for simplicity). The mean for each word distribution is unknown at this point, however, we assign a d-prime parameter (i.e. the signal-noise difference between words and non-words) to each distribution (d’hf, d’lf, d’vlf). The second parameter we denote is the criterion (C) parameter i.e. the point at which an individual responds non-word or word. - if the value is above the criterion, - if the value is below the criterion. Given these parameters, one would expect that the “word” distribution would have a higher mean than the non-words, with some partial overlap (for words and non-words which might be difficult to classify). The criterion should then be set somewhere between these means. If a person was biased to respond “word”, they’re criterion would move down. In total, in the simple SDT model, there would be 2 parameters - d’ (the mean of the word distribution) and C (the criterion). A second example might cover another SDT example with the addition of trial level covariate i.e. analytic solution NA. References "],
["non-rtchoice-example.html", "Chapter 7 Non RT/Choice example", " Chapter 7 Non RT/Choice example "],
["references.html", "References", " References "]
]
