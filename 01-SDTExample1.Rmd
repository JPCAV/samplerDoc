# How to use the PMwG sampler - Signal Detection Theory
```{r, required packages, include=FALSE}
library(tidyverse)
library(knitr)
library(devtools)
```
Here we demonstrate how to use the PMwG sampler package to run a simple signal detection theory (SDT) analysis on a lexical decision task.  We recognise that it is unnecessary to use the sampler package for a simple analysis such as this; however, we hope this example demonstrates the usefulness of the PMwG sampler package.  

### Signal Detection Theory analysis of lexical decision task {#sdtOutline}

We assume you have an understanding of SDT and lexical decision tasks, so we'll jump straight into how you can use the PMwG package with SDT in the context of a lexical decision task.<br>

<b> Do we need to explain lexical decision tasks?? We desribe the procedure briefly below -  Participants were asked to indicate whether a letter string was a <i>word</i> or a <i>non-word</i>. </b>

We begin with the distributions for <i>non-word</i> and <i>word</i> stimuli. You can think of these two distributions as the 'noise' and 'signal' curves, respectively. Each distribution represents the evidence for 'word-likeness' and they are assumed to be normally distributed. The <i>non-word</i> distribution (or the 'noise' distribution) has a mean ($\mu$) of 0 and a standard deviation (SD) of 1. We could estimate SD here, but we will use 1 in this example for simplicity. The mean for the <i>word</i> distribution is unknown at this point; however, we assign a d-prime (d') parameter to denote the difference between the mean of the <i>non-word</i> and the mean of the <i>word</i> distributions (i.e. the 'sensitivity' to word stimuli or the signal-noise difference between <i>words</i> and <i>non-word</i>). As can be seen in Figure \@ref(fig:SDT1), the <i>word</i> distribution mean is greater than the <i>non-word</i> distribution mean (<b>in the positive direction?</b>); however, the distributions partially overlap where <i>non-words</i> and <i>words</i> are difficult to classify.
<br>
```{r SDT1, echo=FALSE, out.width='100%', fig.show='hold', fig.cap='Simple SDT example of lexical decision task'}
knitr::include_graphics('SDT_1.png')
```

The second parameter we denote is the criterion (<b>C</b>) parameter. The criterion is the point at which an individual responds <i>non-word</i> (to the left of <b>C</b> in Figure \@ref(fig:SDT2)) or <i>word</i> (to the right of <b>C</b> in Figure \@ref(fig:SDT2)) and it is set somewhere between the means of the two distributions. If you're biased to respond <i>word</i>, the criterion would move to the left. Conversely, if you're biased to respond <i>non-word</i> then the criterion would move to the right.

```{r SDT2, echo=FALSE, out.width='100%', fig.show='hold', fig.cap='Simple SDT example of lexical decision task'}
knitr::include_graphics('SDT_2.png')
```

<b> Do we need to add something about means should be positive - to the right of NW mean of 0, otherwise the line about "given these parameters, one would expect that the <i>word</i> distribution would have a higher mean than the non-words, with partial overlap" does not make sense.</b>

### Writing a log-likelihood function

Let's write a simple log likelihood function for a fabricated data set. You can copy the code below to follow along with the example.  
```{r setupllfab}
resp <- c("word", "word", "non-word", "word", "non-word", "non-word", "word", "non-word")
stim <- c("word", "word", "non-word", "word", "non-word", "non-word", "non-word", "word")
fabData <- as.data.frame(cbind(resp, stim))
```
We create our dataset by combining a response `resp` and a stimulus `stim` vector into a data frame as shown in \@ref(tab:fakeHead) below.
```{r fakeHead, out.width='80%', fig.asp=.75, fig.align='center', echo=FALSE}
kable(fabData, caption = 'A fabricated dataset of 7 trials with a response and a stimuls column')
```

Our log likelihood function will step through the data, line by line, and find a likelihood value for each trial, under two parameters; d-prime `d` and criterion `C`. 

<br><b> Remove this paragraph? Some info is covered above</b>
As mentioned [above](#sdtOutline) the <i>non-word</i> distribution has a mean of 0 and SD of 1. This then gives a reference point for where the mean of the <i>word</i> distribution would sit and is denoted by dâ€™.
Now we must find the location of the criterion. Setting the criterion allows us to determine which response will be made i.e. above the criterion, participant will respond <i>word</i> and below the criterion, participant will respond <i>non-word</i>.


Here is our complete log likelihood function. We have omitted some code from the code blocks below to enhance appearance, so we encourage you to copy the log likelihood function from the following code block if you'd like to follow along with our example.
```{r sdtLlComplete, attr.source = '.numberLines', eval=FALSE}
SDT_ll <- function(x, data, sample = FALSE){
  if (sample){
    data$response <- NA
  } else{
    out <- numeric(nrow(data))
    data$out <- NA
  }
  if (!sample){
  for (i in 1:nrow(data)){
    if (stim[i] == "word"){
      if (resp[i] == "word"){
        out[i] <- pnorm(x["C"], mean = x["d"], sd = 1, 
                        log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C"], mean = x["d"], sd = 1, 
                        log.p = TRUE, lower.tail = TRUE)
      }
    }else{
      if (resp[i] == "word"){
        out[i] <- pnorm(x["C"], mean = 0, sd = 1, 
                        log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C"], mean = 0, sd = 1, 
                        log.p = TRUE, lower.tail = TRUE)
        }
      }
  }
  sum(out)
  }
}
```


We initialise the log likelihood function with three arguments  
```{r llS1, attr.source = '.numberLines', eval=FALSE}
SDT_loglike <- function(x, data, sample = FALSE) {
```
* `x` is a named parameter vector (e.g.`pars`)
* `data` is the dataset
* `sample =` sample values (For this simple example, we do not require a `sample` argument.)

The first if statement (line 2) checks if you want to sample, this is used for posterior predictive sampling which we will cover in later chapters. and assigns NAs to your data frames response column. If you're not sampling (like us in this example), you need to create an output vector `out`. The `out` vector will contain the log likelihood value for each row/trial in your dataset. <b> Need to explain the `(sample)` part below </b>
```{r sdtllL2, attr.source='.numberLines startFrom="2"', eval=FALSE}
  if (sample){
    data$response <- NA
    }else{
      out <- numeric(nrow(data))
      }
```

From line 9, we check each row in the data set, first considering all trials with <i>word</i> stimuli `if (stim[i] == "word"` (line 10), and assign a likelihood for responding <i>word</i> (line 12-13) or <i>non-word</i> (line 15-16). The <i>word</i> distribution has a mean of `x["d"]` (d-prime parameter) and a decision criterion parameter `x["C"]`. If the response is <i>word</i>, we are considering values ABOVE or to the right of <B>C</B> in \@ref(fig:SDT2), so we set `lower.tail =` to `FALSE`. If the response is <i>non-word</i>, we look for values BELOW or to the left of <B>C</B> in \@ref(fig:SDT2) and we set `lower.tail =` to `TRUE`. The `log.p =` argument takes the log of all likelihood values when set to `TRUE`. We do this so we can sum all likelihoods at the end of the log likelihood function. 
<b> Do we need to explain p-norm??</b>
```{r sdtllL8, attr.source='.numberLines startFrom="8"', eval=FALSE}
  if (!sample){
    for (i in 1:nrow(data)){
      if (stim[i] == "word"){
        if (resp[i] == "word"){
          out[i]<- pnorm(x["C"], mean = x["d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
    }else{
      out[i]<- pnorm(x["C"], mean = x["d"], sd = 1, 
                     log.p = TRUE, lower.tail = TRUE)
      }
```

From the else statement on line 18, we have the function for <i>non-word</i> trials i.e. `stim[i] == "non-word"`. As can be seen below, the output value `out[i]` for these trials is arrived at in a similar manner to the <i>word</i> trials. We set the `mean` to 0 and the standard deviation `sd` to 1. If the response is <i>word</i>, we are considering values ABOVE or to the right of <B>C</B> in \@ref(fig:SDT2), so we set `lower.tail =` to `FALSE`. If the response is <i>non-word</i>, we look for values BELOW or to the left of <B>C</B> in \@ref(fig:SDT2) and we set `lower.tail =` to `TRUE`. Again we want the log of all likelihood values so we set `log.p = TRUE`.

```{r part4.3, attr.source='.numberLines startFrom="18"', eval=FALSE} 
    else{
      if (resp[i] == "word"){
        out[i] <- pnorm(x["C"], mean = 0, sd = 1, 
                        log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i]<- pnorm(x["C"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
```

The final line of code on line 24 sums the `out` vector and returns a log-likelihood value for your model.
<b> The text alignment/justification for code blocks is determined by the length of the longest line in the code block</b>
```{r part4.4,eval=FALSE}
sum(out)
```

## Testing the SDT log likelihood function

Before we run the log likelihood function, we must create a parameter vector `pars` containing the same parameter names used in our log likelihood function above i.e. we name the criterion `C` and d-prime parameter `d` and we assign arbitrary values to each parameter. 

```{r, setupPars}
pars <- c(C = 0.8, d = 2)
```

We can test run our log likelihood function by passing in the parameter vector `pars` and the fabricated dataset we created above `fabData`. 

```{r simple-SDT, include=FALSE}
SDT_loglike <- function(x, data, sample = FALSE){
  if (sample){
    data$response <- NA
  } else{
    out <- numeric(nrow(data))
  }
  if (!sample){
  for (i in 1:nrow(data)){
    if (stim[i] == "word"){
      if (resp[i] == "word"){
        out[i] <- pnorm(x["C"], mean = x["d"], sd = 1, 
                        log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C"], mean = x["d"], sd = 1, 
                        log.p = TRUE, lower.tail = TRUE)
      }
    }else{
      if (resp[i] == "word"){
        out[i] <- pnorm(x["C"], mean = 0, sd = 1, 
                        log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C"], mean = 0, sd = 1, 
                        log.p = TRUE, lower.tail = TRUE)
        }
      }
  }
  sum(out)
  }
}
```

```{r part5}
SDT_loglike(pars, fabData)
```
Now, if we change the parameter values, the log-likelihood value should also change.
```{r part6}
pars <- replace(pars, c(1,2), c(0.5, 1.2))
SDT_loglike(pars, fabData)
```
We can see the log likelihood has changed, so these values are more accurate given the data. 
<b> What does this tell us? Seems insufficient to look for a change in the LL</b>

## SDT log likelihood function for Wagenmakers experiment

Now that we've covered a simple test example, let's create a log likelihood function for the @wagenmakers2008diffusion dataset. 

### Description of Wagenmakers experiment

If you'd like to follow our example, you will need to access the Wagenmakers dataset. This can be done by installing the `rtdists` package and calling the `speed_acc` data frame. The structure of the `speed_acc` dataset will need to be modified in order to meet the requirements of the PMwG sampler. To do this, modify the dataset to match the structure illustrated in table \@ref(tab:wagenmakers10).

Participants were asked to indicate whether a letter string was a <i>word</i> or a <i>non-word</i>.  A subset of Wagenmaker et al data are shown in table \@ref(tab:wagenmakers10), with each line representing a single trial. We have a `subject` column with a subject id (1-19), a condition column `cond` which indicates the proportion of <i>words</i> to <i>non-words</i> presented within a block of trials. In word blocks (`cond = w`) participants completed 75% word and 25% non-word trials and for non-word (`cond = nw`) blocks 75% non-word and 25% word trials. The `stim` column lists the word's frequency i.e. is the stimulus a <i>very low frequency</i> word (`stim = vlf`), a <i>low frequency</i> word (`stim = lf`), a <i>high frequency</i> word (`stim = hf`) or a <i>non-word</i> (`stim = nw`). The third column `resp` refers to the participant's response i.e. the participant responded <i>word</i> (`resp = W`) or <i>non-word</i> (`resp = NW`). The two remaining columns list the response time (`rt`) and whether the paricipant made a correct (`correct = 2`) or incorrect (`correct = 1`) choice. For more details about the experiment please see [the original paper](https://www.sciencedirect.com/science/article/pii/S0749596X07000496).  <b> Is this the correct paper to reference? </b>


```{r setupWagen, include=FALSE}
load("wagenmakers_2008.RData")
wgnmks2008 <- data
```

```{r wagenmakers10, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
kable(slice(wgnmks2008, 91:102), row.names = FALSE, caption = 'Subset of 10 trials from the Wagenmakers (2008) dataset.')
```

Our log-likelihood function for Wagenmakers experimental data is similar to the function we wrote above, except now we require a criterion parameter for each condition and a d-prime parameter for each of the `stim` <i>word</i> types. This is illustrated in figure \@ref(fig:SDT3) below, where we have a non-word criterion <B>C<sub>nw</sub></B>, a word criterion <B>C<sub>w</sub></B> and three distributions for each of the `stim` types with corresponding d-prime for each distribution: <b>d<sub>vlf</sub></b>, <b>d<sub>lf</sub></b>, <b>d<sub>hf</sub></b>.

```{r SDT3, echo=FALSE, out.width='100%', fig.show='hold', fig.cap='Signal detection theory example of lexical decision task'}
knitr::include_graphics('SDT_3.png')
```


Here is our complete log likelihood function for the Wagenmakers data set.

```{r wagenSDT, attr.source = '.numberLines', eval=FALSE}
SDT_ll <- function(x, data, sample=FALSE){
  if (sample){
    data$response <- NA
  } else{
    out <- numeric(nrow(data))
  }
  if (!sample){
  for (i in 1:nrow(data)){
    if (data$cond[i] == "w"){
    if (data$stim[i] == "hf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
      }
    } else if (data$stim[i] == "lf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = TRUE)
          }
        }
    else{
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      }
    }
    else{
      if (data$stim[i] == "hf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }  else if (data$stim[i] == "lf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
      else{
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
    }
  }
  sum(out)
  }
}
```

Line 1 through to line 8 are the same as the log likelihood we wrote for the fabricated dataset above. From line 9, we calculate the log-likelihood `out[i]` for <i>word</i> condition trials `cond[i] == "w"` when the stimulus is a high frequency word `stim[i] == "hf"` for each response. We do this by considering the upper tail of the high frequency word distribution `lower.tail = FALSE`, from the word criterion <B>C<sub>w</sub></B>, for <i>word</i> responses `resp[i] == "W"` and the lower tail for <i>non-word</i> responses (`else` statement on line 14). 

```{r wgnllSlow, attr.source='.numberLines startFrom="9"', eval=FALSE}
 if (data$cond[i] == "w"){
    if (data$stim[i] == "hf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
      }
```

From line 18 and still considering trials from the <i>word</i> condition, we calculate the log likelihood for each response for low frequency words `stim[i] == "lf"`, very low frequency words `stim[i] == "vlf"` (line 26) and non-word stimuli (`else` statement on line 35).

```{r wgnllslow2, attr.source='.numberLines startFrom="18"', eval=FALSE}
  else if (data$stim[i] == "lf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = TRUE)
          }
        }
    else{
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      }
    
```
That covers the <i>word</i> condition. From line 45 through 78, we repeat the same process for the <i>non-word</i> conditon....

```{r wgnllslow3, attr.source='.numberLines startFrom="45"', eval=FALSE}
else{
      if (data$stim[i] == "hf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }  else if (data$stim[i] == "lf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
      else{
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
```

...and sum the values and return the log-likelihood for our model.
```{r part7.7, eval=FALSE}
  sum(out)
```

This give us a log likelihood for all data. Let's test this...

```{r sdt_freq, include=FALSE}
SDT_ll <- function(x, data, sample=FALSE){
  if (sample){
    data$response <- NA
  } else{
    out <- numeric(nrow(data))
  }
  if (!sample){
  for (i in 1:nrow(data)){
    if (data$cond[i] == "w"){
    if (data$stim[i] == "hf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
      }
    } else if (data$stim[i] == "lf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = TRUE)
          }
        }
    else{
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      }
    }
    else{
      if (data$stim[i] == "hf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }  else if (data$stim[i] == "lf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
      else{
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
    }
  }
  sum(out)
  }
}
```

```{r part7.8, include=TRUE}
pars <- log(c(C.w = 1, C.nw = 0.5, HF.d = 3, LF.d = 1.8, VLF.d = 0.7))
SDT_ll(pars, wgnmks2008, sample = FALSE)
```


### Computation time of log likelihood function

You may have noticed that our log likelihood function is slow and heavy on computer time when processing the data trial by trial. We recommend you write a 'slow' log likelihood (as written above) to check it functions as it should before improving the function's efficiency.
<br>
Now we'll speed up our log likelihood function. We have 16 possible values that could be assigned per line in the previous function (for the 16 cells of the design given by proportion (2) x stimuli (4) x response (2)). Rather than looping over each trial, we could calculate the log-likelihood for each cell in the design and multiply the number of instances for each subject. To do this, we add a column to the dataframe by tabling as shown in the code below
```{r part8.1,include=TRUE}
wgnmks2008Fast <- as.data.frame(table(wgnmks2008$subject, wgnmks2008$cond,
                                  wgnmks2008$stim, wgnmks2008$resp))
names(wgnmks2008Fast) <- c("subject", "cond", "stim", "resp", "n")
```

Now our data frame looks like this..
```{r newdata, echo=FALSE}
kable(head(wgnmks2008Fast))
```


For our SDT log likelihood function, we add `n*` (i.e. a multiplying factor) to each of these values to calculate the model log likelihood <b> Should we shorten this code block? Seems excessive.</b>
```{r part8.2,eval=FALSE}
SDT_ll_fast <- function(x, data, sample = FALSE){
  if (!sample){
    out <- numeric(nrow(data))
    for (i in 1:nrow(data)){
      if (data$cond[i]=="w"){
        if (data$stim[i] == "hf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["HF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["HF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }  else if (data$stim[i] == "lf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["LF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["LF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        } else if (data$stim[i] == "vlf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["VLF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["VLF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
        else{
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = 0,
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = 0,
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
      }
      else{
        if (data$stim[i] == "hf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["HF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["HF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }  else if (data$stim[i] == "lf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["LF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["LF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        } else if (data$stim[i] == "vlf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["VLF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["VLF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
        else{
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = 0,
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = 0,
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
      }
    }
    sum(out)
  }
```

Now we have a fast(er) SDT log likelihood function and we can compare its output to the slow log likelihood function's output to make sure it is functioning correctly.

```{r fast_sdt,include=FALSE}
SDT_ll_fast = function(x,data,sample=FALSE){
  #sets up a vector of outputs for when sample = false and erases the data$resp column when sample = true so that it can fill it in later
  if (sample){
    data$resp=NA
  } else{
    out <- numeric(nrow(data))
  }
  #in this example, each subject has only 1 line of data per condition (proportion of words condition (2) x stimulus (4) x response (2) = 16 lines per person )
  # and a value (n) representing the number of responses in that conditions 
  if (!sample){
    for (i in 1:nrow(data)){
      #in total, there are 16 calls to pnorm - which represent the 16 possible outcomes. Each will then only be called once per subject
      #for each line where the proportion was words do this
      if (data$cond[i]=="w"){
        #for each line where proportion was words and the stimulus was a hf word, do this
        if (data$stim[i] == "hf"){
          ##for each line where proportion was words the stimulus was a hf word and the response was "word", do this
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["HF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["HF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }  else if (data$stim[i] == "lf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["LF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["LF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        } else if (data$stim[i] == "vlf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["VLF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["VLF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
        else{
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = 0,sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = 0,sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
      }
      else{
        if (data$stim[i] == "hf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["HF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["HF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }  else if (data$stim[i] == "lf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["LF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["LF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        } else if (data$stim[i] == "vlf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["VLF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["VLF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
        else{
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = 0,sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = 0,sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
      }
    }
    sum(out)
  }else{
    
    for (i in 1:nrow(data)){
      #for sampling - this could be done in a number of ways
      # in this example, the data is looked at line by line (so it must be in the original format) and takes a little longer
      #for this example, I first find the condition (non word or word proportion)
      if (data$cond[i]=="w"){
        #then i find the stimulus (hf,lf,vlf or nw)
        if (data$stim[i] == "hf"){
          #then for each stimulus i do a test using the criterion for that proportion condition (so this one is the word proportion so i use the C.w criterion)
          #using rnorm to pick a random value given a normal distribution with mean HF.d (given the stimulus condition)
          #i test that value against C.w
          #if the value is larger than C.w, the response is "word" otherwise, the simulated response is "non-word"
            data$resp[i]<-ifelse(test=(rnorm(1, mean = x["HF.d"],sd = 1))>x["C.w"], "word","non-word")
        } else if (data$stim[i] == "lf"){
            data$resp[i]<-ifelse(test=(rnorm(1, mean = x["LF.d"],sd = 1))>x["C.w"], "word","non-word")
           } else if (data$stim[i] == "vlf"){
            data$resp[i]<-ifelse(test=(rnorm(1, mean = x["VLF.d"],sd = 1))>x["C.w"], "word","non-word")
        }else{
            data$resp[i]<-ifelse(test=(rnorm(1, mean = 0,sd = 1)) >x["C.w"], "word","non-word")
          }
      } else{
        if (data$stim[i] == "hf"){
            data$resp[i]<-ifelse(test=(rnorm(1, mean = x["HF.d"],sd = 1))>x["C.nw"], "word","non-word")
        } else if (data$stim[i] == "lf"){
           data$resp[i]<-ifelse(test=(rnorm(1, mean = x["LF.d"],sd = 1))>x["C.nw"], "word","non-word")
        } else if (data$stim[i] == "vlf"){
            data$resp[i]<-ifelse(test=(rnorm(1, mean = x["VLF.d"],sd = 1))>x["C.nw"], "word","non-word")
        } else{
            data$resp[i]<-ifelse(test=(rnorm(1, mean = 0,sd = 1))>x["C.nw"], "word","non-word")
          }
        }
    }
    return(data)
    }
}
```
```{r part8.6, include=TRUE}
pars <- log(c(C.w = 1, C.nw = 0.5, HF.d = 3, LF.d = 1.8, VLF.d = 0.7))
SDT_ll(pars, wgnmks2008, sample = FALSE)
SDT_ll_fast(pars, wgnmks2008Fast, sample = FALSE)
```

Great! Both functions produce the same log likelihood! And we can run one final check by modifying the parameter vector's values

```{r part8.7,include=TRUE}
pars <- log(c(C.w = 1, C.nw = 0.8, HF.d = 2.7, LF.d = 1.8, VLF.d = 1.3))
SDT_ll(pars, wgnmks2008, sample = FALSE)
SDT_ll_fast(pars, wgnmks2008Fast, sample = FALSE)
```

<b> We recommend "speeding up" your code however you wish. We do it in the Forstmann code.... </b>.  When you're confident that your log likelihood functions correctly, you should save it as a separate script so it can be sourced when running the sampler.


## PMwG Framework

Now that we have written a log likelihood function, we're ready to use the PMwG sampler package. 

Let's begin by installing the PMwG samplers package. <b>We currently recommended installing psamplers via devtools.</b>
```{r instlPMwGpkg, eval=FALSE}
# The samplers package will be on CRAN - this step will be removed.
install_github('newcastlecl\samplers')
```
```{r loadPMwGpkg}
library(psamplers)
```
Now we require the parameter vector `pars` we specified above and a priors object called `priors`. The `priors` object is a list that contains two components:



* `theta_mu` a vector containing the prior for model parameter means
* `theta_sig` the prior covariance matrix for model parameters.

```{r part9.3, include=TRUE}
pars <- c("C.w","C.nw","HF.d","LF.d","VLF.d") # This is the same as the `pars` vector specified above
priors <- list(
  theta_mu = rep(0, length(pars)),
  theta_sig = diag(rep(1, length(pars)))
)

```
The `priors` object in our example is initiated with zeros. <b>Under what conditions would this priors object  differ?</b>


The next step is to load your log likelihood function/script.

```{r loadYaLL, echo=TRUE, eval=FALSE}
source(file = "yourLogLikelihoodFile.R")
```

Once you've setup your parameters, priors and written a log likelihood function, the next step is to initialise the `sampler` object. 
```{r SDTsamplerObject, echo=TRUE, eval=FALSE}
sampler <- pmwgs(
  data = wgnmks2008Fast,
  pars = pars,
  prior = priors,
  ll_func = SDT_ll_fast
)
```

The `pmwgs` function takes a set of arguments (listed below) and returns a list containing the required components for performing the particle metropolis within Gibbs steps.

* `data =`a data frame (e.g.`wgnmks2008Fast`) with a column for participants called <b>`subject`</b>
* `pars =` the model parameters to be used (e.g.`pars`)
* `prior =` the priors to be used (e.g.`priors`)
* `ll_func =` name of log likelihood function you've sourced above (e.g.`SDT_ll_fast`)


```{r part9.4, include=TRUE}
sampler <- pmwgs(
  data = wgnmks2008Fast,
  pars = pars,
  prior = priors,
  ll_func = SDT_ll_fast
)
```


### Model start points {#start-points}
You have the option to set model start points. We use 0 for the mean (mu) and a variance of 0.01. If you chose not to specify start points, the sampler will randomly sample points from the prior distribution.
```{r part9.5, include=TRUE}
start_points <- list(
  mu = rep(0, length.out = length(pars)),
  sig2 = diag(rep(.01, length(pars)))
)
```

The `start_points` object contains two vectors:

* `mu` a vector of start points for the mu of each model parameter 
* `sig2` vector containing the start points of the covariance matrix of covariance between model parameters.


### Running the sampler {#run-sdtsampler}
Okay - now we are ready to run the sampler.  
```{r SDTrunSampler, echo=TRUE, eval=FALSE}
sampler <- init(sampler, theta_mu = start_points$mu,
                theta_sig = start_points$sig2)
```
Here we are using the `init` function to generate initial start points for the random effects and storing them in the `sampler` object.  First we pass the `sampler` object from above that includes our data, parameters, priors and log likelihood function.  If we decided to specify our own start points (as above), we would include the `theta_mu` and `theta_sig` arguments.


Now we can run the sampler using the `run_stage` function. The `run_stage` function takes four arguments:

* `x` the `sampler` object including parameters 
* `stage =` the sampling stage (e.g. `"burn"`, `"adapt"` or `"sample"`)
* `iter = ` is the number of iterations for the sampling stage 
* `particles =` is the number of particles generated on each iteration
* `display_progress =` shows progress bar of current stage
* `n_cores =` numner of processor cores to run stage on
* `epsilon = ` must be >0<1 <b> We will write something here </b>

It is optional to include the `iter =` and `particles =` arguments. If these are not included, `iter` and `particles` default to 1000. The number of iterations you choose for your burn in stage is similar to choices made when running deMCMC, however, this varies depending on the time the model takes to reach the 'real' posterior space. 

First we run our burn-in stage by setting `stage =` to `"burn"`. 
```{r SDTburn, echo=TRUE, eval=FALSE}
burned <- run_stage(sampler, stage = "burn", iter = 1000, particles = 20, display_progress = TRUE, n_cores = 8)
```

Now we run our adaptation stage by setting `stage = "adapt"`. This function creates an efficient proposal distribution. The sampler will attempt to create the proposal distribution after 20 unique particles have been accepted for each subject. The sampler will then test whether the distribution was able to be created and if it was created, the sampler will move to the next stage otherwise the sampler will continue to sample. The number of iterations needs to be great enough to generate enough unique samples, but not too large as to make..... <b>Check this with Scott/Guy</b> 
```{r SDTadaptation, echo=TRUE, eval=FALSE}
adapted <- run_stage(burned, stage = "adapt", iter = 1000, particles = 20, n_cores = 8)
```

At the start of the `sampled` stage, the sampler object will create a 'proposal' distribution for each subject's random effects using a conditional multi-variate normal. This proposal distribution is then used to efficiently generate new particles for each subject which means we can reduce the number of particles on each iteration whilst still achieving acceptance rates.
```{r SDTsampled, echo=TRUE, eval=FALSE}
sampled <- run_stage(adapted, stage = "sample", iter = 1000, particles = 20, n_cores = 8)
```


## Check sampling process

It is a good idea to check your samples by producing some simple plots as shown below. The first plot gives an indication of the chains for the group level parameters. In this example, you will see the chains take only several iterations before arriving at the posterior, however, this may not always be the case. Each parameter chain (lines on plot \@ref(fig:parPlot)) should be stationary i.e. the chain should not trend up or down, and once the chain reaches the posterior, the chain should remain relatively 'thin'. If the chain is wide or continually jump between large values (e.g. move between -3 and 3) then there is likely an error in your log likelihood function. 

<b>Check with Reilly sentence below</b>

As you can see in \@ref(fig:parPlot), the chains are clearly separate and are stable (not trending in any direction), as well as being relatively thin (largest variance ~1).

```{r load_data, include=FALSE}
load("SDT_sampled.Rdata")
```


```{r parPlot, echo=FALSE, out.width='100%', fig.show='hold', fig.cap='Posterior samples of parameters'}
matplot(t(sampled$samples$theta_mu),type="l")
```

The second plot  below (\@ref(fig:subjLLPlot)) shows the likelihoods across iterations for each subject. Again we see that the likelihood values jump up after only a few iterations and then remain stable, with only slight movement. 


```{r subjLLPlot, echo=FALSE, out.width='100%', fig.show='hold', fig.cap='Posterior samples of subject log likelihoods'}
matplot(t(sampled$samples$subj_ll),type="l")
```


## Simulating Posterior Data

