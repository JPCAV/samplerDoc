# How to use the PMwG sampler - Signal Detection Theory
```{r, required packages, include=FALSE}
library(tidyverse)
library(knitr)
library(devtools)
```
Here we demonstrate how to use the PMwG sampler package to run a simple signal detection theory (SDT) analysis on a lexical decision task.  We recognise that it is unnecessary to use the sampler package for a simple analysis such as this; however, we hope this example demonstrates the usefulness of the PMwG sampler package.  

### Signal Detection Theory analysis of lexical decision task {#sdtOutline}

We assume you have an understanding of SDT and lexical decision tasks, so we'll jump straight into how you can use the PMwG package with SDT in the context of a lexical decision task.<br>

<b> Do we need to explain lexical decision tasks?? We desribe the procedure briefly below -  Participants were asked to indicate whether a letter string was a <i>word</i> or a <i>non-word</i>. </b>

We begin with the distributions for <i>non-word</i> and <i>word</i> stimuli. You can think of these two distributions as the 'noise' and 'signal' curves, respectively. Each distribution represents the evidence for 'word-likeness' and they are assumed to be normally distributed. The <i>non-word</i> distribution (or the 'noise' distribution) has a mean ($\mu$) of 0 and a standard deviation (SD) of 1. We could estimate SD here, but we will use 1 in this example for simplicity. The mean for the <i>word</i> distribution is unknown at this point; however, we assign a d-prime (d') parameter to denote the difference between the mean of the <i>non-word</i> and the mean of the <i>word</i> distributions (i.e. the 'sensitivity' to word stimuli or the signal-noise difference between <i>words</i> and <i>non-word</i>). As can be seen in Figure \@ref(fig:SDT1), the <i>word</i> distribution mean is greater than the <i>non-word</i> distribution mean (<b>in the positive direction?</b>); however, the distributions partially overlap where <i>non-words</i> and <i>words</i> are difficult to classify.
<br>
```{r SDT1, echo=FALSE, out.width='100%', fig.show='hold', fig.cap='Simple SDT example of lexical decision task'}
knitr::include_graphics('SDT_1.png')
```

The second parameter we denote is the criterion (<b>C</b>) parameter. The criterion is the point at which an individual responds <i>non-word</i> (to the left of <b>C</b> in Figure \@ref(fig:SDT2)) or <i>word</i> (to the right of <b>C</b> in Figure \@ref(fig:SDT2)) and it is set somewhere between the means of the two distributions. If you're biased to respond <i>word</i>, the criterion would move to the left. Conversely, if you're biased to respond <i>non-word</i> then the criterion would move to the right.

```{r SDT2, echo=FALSE, out.width='100%', fig.show='hold', fig.cap='Simple SDT example of lexical decision task'}
knitr::include_graphics('SDT_2.png')
```

<b> Do we need to add something about means should be positive - to the right of NW mean of 0, otherwise the line about "given these parameters, one would expect that the <i>word</i> distribution would have a higher mean than the non-words, with partial overlap" does not make sense.</b>

### Writing a log-likelihood function

Let's write a simple log likelihood function for a fabricated data set. You can copy the code below to follow along with the example.  
```{r setupllfab}
resp <- c("word", "word", "non-word", "word", "non-word", "non-word", "word", "non-word")
stim <- c("word", "word", "non-word", "word", "non-word", "non-word", "non-word", "word")
fabData <- as.data.frame(cbind(resp, stim))
```
We create our dataset by combining a response `resp` and a stimulus `stim` vector into a data frame as shown in \@ref(tab:fakeHead) below.
```{r fakeHead, out.width='80%', fig.asp=.75, fig.align='center', echo=FALSE}
kable(fabData, caption = 'A fabricated dataset of 7 trials with a response and a stimuls column')
```

Our log likelihood function will step through the data, line by line, and find a likelihood value for each trial, under two parameters; d-prime `d` and criterion `C`. 

<br><b> Remove this paragraph? Some info is covered above</b>
As mentioned [above](#sdtOutline) the <i>non-word</i> distribution has a mean of 0 and SD of 1. This then gives a reference point for where the mean of the <i>word</i> distribution would sit and is denoted by dâ€™.
Now we must find the location of the criterion. Setting the criterion allows us to determine which response will be made i.e. above the criterion, participant will respond <i>word</i> and below the criterion, participant will respond <i>non-word</i>.


Here is our complete log likelihood function. We have omitted some code from the code blocks below to enhance appearance, so we encourage you to copy the log likelihood function from the following code block if you'd like to follow along with our example.
```{r sdtLlComplete, attr.source = '.numberLines', eval=FALSE}
SDT_ll <- function(x, data, sample = FALSE){
  if (sample){
    data$response <- NA
  } else{
    out <- numeric(nrow(data))
    data$out <- NA
  }
  if (!sample){
  for (i in 1:nrow(data)){
    if (stim[i] == "word"){
      if (resp[i] == "word"){
        out[i] <- pnorm(x["C"], mean = x["d"], sd = 1, 
                        log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C"], mean = x["d"], sd = 1, 
                        log.p = TRUE, lower.tail = TRUE)
      }
    }else{
      if (resp[i] == "word"){
        out[i] <- pnorm(x["C"], mean = 0, sd = 1, 
                        log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C"], mean = 0, sd = 1, 
                        log.p = TRUE, lower.tail = TRUE)
        }
      }
  }
  sum(out)
  }
}
```


We initialise the log likelihood function with three arguments  
```{r llS1, attr.source = '.numberLines', eval=FALSE}
SDT_loglike <- function(x, data, sample = FALSE) {
```
* `x` is a named parameter vector (e.g.`pars`)
* `data` is the dataset
* `sample =` sample values (For this simple example, we do not require a `sample` argument.)

The first if statement (line 2) checks if you want to sample, this is used for posterior predictive sampling which we will cover in later chapters. and assigns NAs to your data frames response column. If you're not sampling (like us in this example), you need to create an output vector `out`. The `out` vector will contain the log likelihood value for each row/trial in your dataset. <b> Need to explain the `(sample)` part below </b>
```{r sdtllL2, attr.source='.numberLines startFrom="2"', eval=FALSE}
  if (sample){
    data$response <- NA
    }else{
      out <- numeric(nrow(data))
      }
```

From line 9, we check each row in the data set, first considering all trials with <i>word</i> stimuli `if (stim[i] == "word"` (line 10), and assign a likelihood for responding <i>word</i> (line 12-13) or <i>non-word</i> (line 15-16). The <i>word</i> distribution has a mean of `x["d"]` (d-prime parameter) and a decision criterion parameter `x["C"]`. If the response is <i>word</i>, we are considering values ABOVE or to the right of <B>C</B> in \@ref(fig:SDT2), so we set `lower.tail =` to `FALSE`. If the response is <i>non-word</i>, we look for values BELOW or to the left of <B>C</B> in \@ref(fig:SDT2) and we set `lower.tail =` to `TRUE`. The `log.p =` argument takes the log of all likelihood values when set to `TRUE`. We do this so we can sum all likelihoods at the end of the log likelihood function. 
<b> Do we need to explain p-norm??</b>
```{r sdtllL8, attr.source='.numberLines startFrom="8"', eval=FALSE}
  if (!sample){
    for (i in 1:nrow(data)){
      if (stim[i] == "word"){
        if (resp[i] == "word"){
          out[i]<- pnorm(x["C"], mean = x["d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
    }else{
      out[i]<- pnorm(x["C"], mean = x["d"], sd = 1, 
                     log.p = TRUE, lower.tail = TRUE)
      }
```

From the else statement on line 18, we have the function for <i>non-word</i> trials i.e. `stim[i] == "non-word"`. As can be seen below, the output value `out[i]` for these trials is arrived at in a similar manner to the <i>word</i> trials. We set the `mean` to 0 and the standard deviation `sd` to 1. If the response is <i>word</i>, we are considering values ABOVE or to the right of <B>C</B> in \@ref(fig:SDT2), so we set `lower.tail =` to `FALSE`. If the response is <i>non-word</i>, we look for values BELOW or to the left of <B>C</B> in \@ref(fig:SDT2) and we set `lower.tail =` to `TRUE`. Again we want the log of all likelihood values so we set `log.p = TRUE`.

```{r part4.3, attr.source='.numberLines startFrom="18"', eval=FALSE} 
    else{
      if (resp[i] == "word"){
        out[i] <- pnorm(x["C"], mean = 0, sd = 1, 
                        log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i]<- pnorm(x["C"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
```

The final line of code on line 24 sums the `out` vector and returns a log-likelihood value for your model.
<b> The text alignment/justification for code blocks is determined by the length of the longest line in the code block</b>
```{r part4.4,eval=FALSE}
sum(out)
```

## Testing the SDT log likelihood function

Before we run the log likelihood function, we must create a parameter vector `pars` containing the same parameter names used in our log likelihood function above i.e. we name the criterion `C` and d-prime parameter `d` and we assign arbitrary values to each parameter. 

```{r, setupPars}
pars <- c(C = 0.8, d = 2)
```

We can test run our log likelihood function by passing in the parameter vector `pars` and the fabricated dataset we created above `fabData`. 

```{r simple-SDT, include=FALSE}
SDT_loglike <- function(x, data, sample = FALSE){
  if (sample){
    data$response <- NA
  } else{
    out <- numeric(nrow(data))
  }
  if (!sample){
  for (i in 1:nrow(data)){
    if (stim[i] == "word"){
      if (resp[i] == "word"){
        out[i] <- pnorm(x["C"], mean = x["d"], sd = 1, 
                        log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C"], mean = x["d"], sd = 1, 
                        log.p = TRUE, lower.tail = TRUE)
      }
    }else{
      if (resp[i] == "word"){
        out[i] <- pnorm(x["C"], mean = 0, sd = 1, 
                        log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C"], mean = 0, sd = 1, 
                        log.p = TRUE, lower.tail = TRUE)
        }
      }
  }
  sum(out)
  }
}
```

```{r part5}
SDT_loglike(pars, fabData)
```
Now, if we change the parameter values, the log-likelihood value should also change.
```{r part6}
pars <- replace(pars, c(1,2), c(0.5, 1.2))
SDT_loglike(pars, fabData)
```
We can see the log likelihood has changed, so these values are more accurate given the data. 
<b> What does this tell us? Seems insufficient to look for a change in the LL</b>

## SDT log likelihood function for Wagenmakers experiment

Now that we've covered a simple test example, let's create a log likelihood function for the @wagenmakers2008diffusion dataset. Before we do that, we need to cover the study design.

### Description of Wagenmakers experiment



Participants were asked to indicate whether a letter string was a <i>word</i> or a <i>non-word</i>.  A subset of Wagenmaker et al data are shown in table \@ref(tab:wagenmakers10), with each line representing a single trial. We have a `subject` column with a subject id (1-19), a condition column `cond` which indicates the proportion of <i>words</i> to <i>non-words</i> presented within a block of trials. In word blocks (`cond = w`) participants completed 75% word and 25% non-word trials and for non-word (`cond = nw`) blocks 75% non-word and 25% word trials. The `stim` column lists the word's frequency i.e. is the stimulus a <i>very low frequency</i> word (`stim = vlf`), a <i>low frequency</i> word (`stim = lf`), a <i>high frequency</i> word (`stim = hf`) or a <i>non-word</i> (`stim = nw`). The third column `resp` refers to the participant's response i.e. the participant responded <i>word</i> (`resp = W`) or <i>non-word</i> (`resp = NW`). The two remaining columns list the response time (`rt`) and whether the paricipant made a correct (`correct = 2`) or incorrect (`correct = 1`) choice. 

If you choose to re-create or follow this example, please note that we have modified the columns names in the Wagenmakers dataset as shown in the table below. For more details about the experiment please see [the original paper](https://www.sciencedirect.com/science/article/pii/S0749596X07000496).  <b> Is this the correct paper to reference? </b>


```{r setupWagen, include=FALSE}
load("wagenmakers_2008.RData")
wgnmks2008 <- data
```

```{r wagenmakers10, echo=FALSE, out.width='80%', fig.asp=.75, fig.align='center'}
kable(slice(wgnmks2008, 91:102), row.names = FALSE, caption = 'Subset of 10 trials from the Wagenmakers (2008) dataset.')
```

Our log-likelihood function for Wagenmakers experimental data is similar to the function we wrote above, except now we require a criterion parameter for each condition and a d-prime parameter for each of the `stim` <i>word</i> types. This is illustrated in figure \@ref(fig:SDT3) below, where we have a non-word criterion <B>C<sub>nw</sub></B>, a word criterion <B>C<sub>w</sub></B> and three distributions for each of the `stim` types with corresponding d-prime for each distribution: <b>d<sub>vlf</sub></b>, <b>d<sub>lf</sub></b>, <b>d<sub>hf</sub></b>.

```{r SDT3, echo=FALSE, out.width='100%', fig.show='hold', fig.cap='Signal detection theory example of lexical decision task'}
knitr::include_graphics('SDT_3.png')
```


Here is our complete log likelihood function for the Wagenmakers data set.

```{r wagenSDT, attr.source = '.numberLines', eval=FALSE}
SDT_ll <- function(x, data, sample=FALSE){
  if (sample){
    data$response <- NA
  } else{
    out <- numeric(nrow(data))
  }
  if (!sample){
  for (i in 1:nrow(data)){
    if (data$cond[i] == "w"){
    if (data$stim[i] == "hf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
      }
    } else if (data$stim[i] == "lf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = TRUE)
          }
        }
    else{
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      }
    }
    else{
      if (data$stim[i] == "hf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }  else if (data$stim[i] == "lf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
      else{
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
    }
  }
  sum(out)
  }
}
```

Line 1 through to line 8 are the same as the log likelihood we wrote for the fabricated dataset above. From line 9, we calculate the log-likelihood `out[i]` for <i>word</i> condition trials `cond[i] == "w"` when the stimulus is a high frequency word `stim[i] == "hf"` for each response. We do this by considering the upper tail of the high frequency word distribution `lower.tail = FALSE`, from the word criterion <B>C<sub>w</sub></B>, for <i>word</i> responses `resp[i] == "W"` and the lower tail for <i>non-word</i> responses (`else` statement on line 14). 

```{r wgnllSlow, attr.source='.numberLines startFrom="9"', eval=FALSE}
 if (data$cond[i] == "w"){
    if (data$stim[i] == "hf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
      }
```

From line 18 and still considering trials from the <i>word</i> condition, we calculate the log likelihood for each response for low frequency words `stim[i] == "lf"`, very low frequency words `stim[i] == "vlf"` (line 26) and non-word stimuli (`else` statement on line 35).

```{r wgnllslow2, attr.source='.numberLines startFrom="18"', eval=FALSE}
  else if (data$stim[i] == "lf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = TRUE)
          }
        }
    else{
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      }
    
```
That covers the <i>word</i> condition. From line 45 through 78, we repeat the same process for the <i>non-word</i> conditon....

```{r wgnllslow3, attr.source='.numberLines startFrom="45"', eval=FALSE}
else{
      if (data$stim[i] == "hf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }  else if (data$stim[i] == "lf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
      else{
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
```

...and sum the values and return the log-likelihood for our model.
```{r part7.7, eval=FALSE}
  sum(out)
```

This give us a log likelihood for all data. Let's test this...

```{r sdt_freq, include=FALSE}
SDT_ll <- function(x, data, sample=FALSE){
  if (sample){
    data$response <- NA
  } else{
    out <- numeric(nrow(data))
  }
  if (!sample){
  for (i in 1:nrow(data)){
    if (data$cond[i] == "w"){
    if (data$stim[i] == "hf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["HF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
      }
    } else if (data$stim[i] == "lf"){
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = x["LF.d"], sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.w"], mean = x["VLF.d"], sd = 1,
                         log.p = TRUE, lower.tail = TRUE)
          }
        }
    else{
      if (data$resp[i] == "W"){
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = FALSE)
      }else{
        out[i] <- pnorm(x["C.w"], mean = 0, sd = 1,
                       log.p = TRUE, lower.tail = TRUE)
        }
      }
    }
    else{
      if (data$stim[i] == "hf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["HF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }  else if (data$stim[i] == "lf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["LF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      } else if (data$stim[i] == "vlf"){
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = x["VLF.d"], sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
      else{
        if (data$resp[i] == "W"){
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = FALSE)
        }else{
          out[i] <- pnorm(x["C.nw"], mean = 0, sd = 1, 
                         log.p = TRUE, lower.tail = TRUE)
        }
      }
    }
  }
  sum(out)
  }
}
```

```{r part7.8, include=TRUE}
pars <- log(c(C.w = 1, C.nw = 0.5, HF.d = 3, LF.d = 1.8, VLF.d = 0.7))
SDT_ll(pars, wgnmks2008, sample = FALSE)
```


### Computation time of log likelihood function

You may have noticed that our log likelihood function is slow and heavy on computer time when processing the data trial by trial. We recommend you write a 'slow' log likelihood (as written above) to check it functions as it should before improving the function's efficiency.
<br>
Now we'll speed up our log likelihood function. We have 16 possible values that could be assigned per line in the previous function (for the 16 cells of the design given by proportion (2) x stimuli (4) x response (2)). Rather than looping over each trial, we could calculate the log-likelihood for each cell in the design and multiply the number of instances for each subject. To do this, we add a column to the dataframe by tabling as shown in the code below
```{r part8.1,include=TRUE}
wgnmks2008Fast <- as.data.frame(table(wgnmks2008$subject, wgnmks2008$cond,
                                  wgnmks2008$stim, wgnmks2008$resp))
names(wgnmks2008Fast) <- c("subject", "cond", "stim", "resp", "n")
```

Now our data frame looks like this..
```{r newdata, echo=FALSE}
kable(head(wgnmks2008Fast))
```


For our SDT log likelihood function, we add `n*` (i.e. a multiplying factor) to each of these values to calculate the model log likelihood
```{r part8.2,eval=FALSE}
SDT_ll_fast <- function(x, data, sample = FALSE){
  if (!sample){
    out <- numeric(nrow(data))
    for (i in 1:nrow(data)){
      if (data$cond[i]=="w"){
        if (data$stim[i] == "hf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["HF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["HF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }  else if (data$stim[i] == "lf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["LF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["LF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        } else if (data$stim[i] == "vlf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["VLF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["VLF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
        else{
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = 0,
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = 0,
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
      }
      else{
        if (data$stim[i] == "hf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["HF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["HF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }  else if (data$stim[i] == "lf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["LF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["LF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        } else if (data$stim[i] == "vlf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["VLF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["VLF.d"],
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
        else{
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = 0,
                                     sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = 0,
                                     sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
      }
    }
    sum(out)
  }
```

Now we have a fast(er) SDT log likelihood function and we can compare its output to the slow log likelihood function's output to make sure it is functioning correctly.

```{r fast_sdt,include=FALSE}
SDT_ll_fast = function(x,data,sample=FALSE){
  #sets up a vector of outputs for when sample = false and erases the data$resp column when sample = true so that it can fill it in later
  if (sample){
    data$resp=NA
  } else{
    out <- numeric(nrow(data))
  }
  #in this example, each subject has only 1 line of data per condition (proportion of words condition (2) x stimulus (4) x response (2) = 16 lines per person )
  # and a value (n) representing the number of responses in that conditions 
  if (!sample){
    for (i in 1:nrow(data)){
      #in total, there are 16 calls to pnorm - which represent the 16 possible outcomes. Each will then only be called once per subject
      #for each line where the proportion was words do this
      if (data$cond[i]=="w"){
        #for each line where proportion was words and the stimulus was a hf word, do this
        if (data$stim[i] == "hf"){
          ##for each line where proportion was words the stimulus was a hf word and the response was "word", do this
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["HF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["HF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }  else if (data$stim[i] == "lf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["LF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["LF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        } else if (data$stim[i] == "vlf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["VLF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = x["VLF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
        else{
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = 0,sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.w"], mean = 0,sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
      }
      else{
        if (data$stim[i] == "hf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["HF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["HF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }  else if (data$stim[i] == "lf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["LF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["LF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        } else if (data$stim[i] == "vlf"){
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["VLF.d"],sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = x["VLF.d"],sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
        else{
          if (data$resp[i] == "W"){
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = 0,sd = 1, log.p = TRUE, lower.tail = FALSE)
          }else{
            out[i]<- data$n[i]*pnorm(x["C.nw"], mean = 0,sd = 1, log.p = TRUE, lower.tail = TRUE)
          }
        }
      }
    }
    sum(out)
  }else{
    
    for (i in 1:nrow(data)){
      #for sampling - this could be done in a number of ways
      # in this example, the data is looked at line by line (so it must be in the original format) and takes a little longer
      #for this example, I first find the condition (non word or word proportion)
      if (data$cond[i]=="w"){
        #then i find the stimulus (hf,lf,vlf or nw)
        if (data$stim[i] == "hf"){
          #then for each stimulus i do a test using the criterion for that proportion condition (so this one is the word proportion so i use the C.w criterion)
          #using rnorm to pick a random value given a normal distribution with mean HF.d (given the stimulus condition)
          #i test that value against C.w
          #if the value is larger than C.w, the response is "word" otherwise, the simulated response is "non-word"
            data$resp[i]<-ifelse(test=(rnorm(1, mean = x["HF.d"],sd = 1))>x["C.w"], "word","non-word")
        } else if (data$stim[i] == "lf"){
            data$resp[i]<-ifelse(test=(rnorm(1, mean = x["LF.d"],sd = 1))>x["C.w"], "word","non-word")
           } else if (data$stim[i] == "vlf"){
            data$resp[i]<-ifelse(test=(rnorm(1, mean = x["VLF.d"],sd = 1))>x["C.w"], "word","non-word")
        }else{
            data$resp[i]<-ifelse(test=(rnorm(1, mean = 0,sd = 1)) >x["C.w"], "word","non-word")
          }
      } else{
        if (data$stim[i] == "hf"){
            data$resp[i]<-ifelse(test=(rnorm(1, mean = x["HF.d"],sd = 1))>x["C.nw"], "word","non-word")
        } else if (data$stim[i] == "lf"){
           data$resp[i]<-ifelse(test=(rnorm(1, mean = x["LF.d"],sd = 1))>x["C.nw"], "word","non-word")
        } else if (data$stim[i] == "vlf"){
            data$resp[i]<-ifelse(test=(rnorm(1, mean = x["VLF.d"],sd = 1))>x["C.nw"], "word","non-word")
        } else{
            data$resp[i]<-ifelse(test=(rnorm(1, mean = 0,sd = 1))>x["C.nw"], "word","non-word")
          }
        }
    }
    return(data)
    }
}
```
```{r part8.6, include=TRUE}
pars <- log(c(C.w = 1, C.nw = 0.5, HF.d = 3, LF.d = 1.8, VLF.d = 0.7))
SDT_ll(pars, wgnmks2008, sample = FALSE)
SDT_ll_fast(pars, wgnmks2008Fast, sample = FALSE)
```

Great! Both functions produce the same log likelihood! And we can run one final check by modifying the parameter vector's values

```{r part8.7,include=TRUE}
pars <- log(c(C.w = 1, C.nw = 0.8, HF.d = 2.7, LF.d = 1.8, VLF.d = 1.3))
SDT_ll(pars, wgnmks2008, sample = FALSE)
SDT_ll_fast(pars, wgnmks2008Fast, sample = FALSE)
```

<b> We recommend "speeding up" your code however you wish. We do it in the Forstmann code.... </b>






















\@ref(fig:sdtPlot)
```{r SDTplotsetup, eval=TRUE, include=FALSE}
library("tidyverse")
library("reshape2")
# Samplers project - SDT example
parEst <- c(C.w = 0.9269677, C.nw = 1.7675702, HF.d = 2.9327993, LF.d = 2.6904818, VLF.d = 2.1823751)
# estimate curve for non-words
nwD <- round(rnorm(n = 5000, mean = 0,sd = 1), 2)
# estimate curve for high frequency words
hfwD <- round(rnorm(n = 5000, mean = parEst[3], sd = 1), 2)
# estimate curve for low frequency words
lfwD <- round(rnorm(n = 5000, mean = parEst[4], sd = 1), 2)
# estimate curve for very low frequency words
vlfwD <- round(rnorm(n = 5000, mean = parEst[5], sd = 1), 2)
# combine into data frame
dfff <- bind_cols(hf = hfwD, lf = lfwD, nw = nwD, vlf = vlfwD)
dfff <- melt(dfff)
```
```{r sdtPlot, echo=FALSE, fig.cap="sdt posterior"}
# plot distributions with non-word criterion (red line) and word criterion (blue line)
ggplot(dfff,aes(x=value, fill=variable)) + 
  geom_density(alpha=0.25) + 
  geom_vline(xintercept = parEst[1], colour = 'blue') + 
  geom_vline(xintercept = parEst[2], colour = 'red') + 
  scale_y_continuous(limits = c(0, 0.42), expand = expand_scale(mult = c(0, .1)))
```

<b>A second example might cover another SDT example with the addition of trial level covariate i.e. analytic solution NA.</b>
