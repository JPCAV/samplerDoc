# PMwG sampler with the Linear Ballistic Accumulator and a complex experiment design {.tabset .tabset-fade}

In [chapter 1](#sdtOutline) we demonstrated how the PMwG sampler can be used to model a lexical decision task in a signal detection framework. However, the SDT framework does not allow us to consider response time (RT) and the join distribution of RT and accuracy. In this example we will expand on what was covered in chapter 1, by fitting the LBA - a more complex model - to the Wagenmakers 2008 data. <!-- see IS square paper experiment 2-->

A description of the Wagenmakers experiment and data is covered in [chapter 1](#sdtOutline). This experiment is more complicated than the Forstmann example in [chapter 3](#forstmannChapter), and the LBA is also more complicated than the SDT model. As a result, the log-likelihood function for this example will be more complex, however, you'll notice that each step closely follows those taken in previous chapters with simpler data sets and simpler models. 


```{r chpt4data, include = FALSE}
load("wagenmakers_2008.RData")
```

## Writing the LBA log-likelihood function for the Wagenmakers data set 

Here we have written a computationally-slow and easy to follow log-likelihood function. The log-likelihood function steps through the data line by line (i.e. trial by trial) and gives a likelihood value for each line given `x` parameters. As mentioned in previous chapters, we encourage those who have experience writing likelihood functions to write a computationally efficient function or use our '<b><i>fast</i></b>function "HERE". For those new to modelling, the trialwise function directly below is easier to follow, debug and is less likely to result in errors. 

The structure of our log-likelihood function follows those in the preceding chapters, so we will focus on the parts that differ i.e. the experiment design and hypothesis about which parameters are being influenced by the experimental manipulations. See [chapter 3](#LBAParameters) for a reminder of the LBA's parameters.


Let's begin by loading `rtdists` package...

```{r loadrtdists}
library(rtdists)
```

...and now a complete trialwise (slow) log-likelihood function.
```{r wagtwll, attr.source = '.numberLines', include=TRUE }
tw_lba_ll <- function(x, data, sample = FALSE) {
  x <- exp(x)
  if (any(data$rt < x["t0"])) {
    return(-1e10)
  }
  
  if (sample) {
    data$rt <- rep(NA, nrow(data))
    data$resp <- rep(NA, nrow(data))
  } else {
    out <- numeric(nrow(data))
  }
  
  for (i in 1:nrow(data)) {
    A = x["A"]
    b.w = x[paste0("b.", data$cond[i], ".W")] + A
    b.nw = x[paste0("b.", data$cond[i], ".NW")] + A
    bs = list(b.nw, b.w)
    v.w = x[paste0("v.", data$stim[i], ".W")]
    v.nw = x[paste0("v.", data$stim[i], ".NW")]
    vs = c(v.nw, v.w)
    t0 = x["t0"]
    s = c(1, 1)
    
    if (sample) {
      tmp <- rLBA(n = 1,
                  A = A,
                  b = bs,
                  mean_v = vs,
                  sd_v = s,
                  t0 = t0,
                  dist = "norm",
                  silent = TRUE
      )
      data$rt[i] <- tmp$rt
      data$resp[i] <- tmp$resp
    } else {
      out[i] <- dLBA(rt = data$rt[i],
                     response = data$resp[i],
                     A = A,
                     b = bs,
                     mean_v = vs,
                     sd_v = s,
                     t0 = t0,
                     dist = "norm",
                     silent = TRUE
                     )
      }
  }
  if (sample) {
    return(data)
  } else {
    bad <- (out < 1e-10) | (!is.finite(out))
    out[bad] <- 1e-10
    out <- sum(log(out))
    return(out)
  }
}
```

<b>Note:</b> If you'd like to run through this example, it is best to copy the `tw_lba_ll` function from the code block above rather than copying from the separate code chunks below where curly braces have been removed.

We begin from the `for` loop on line 14. For for each row in the dataset, we assign the values (`x`) of each parameter in our model so that any conditional parameters (for example `b` in our model) are correctly assigned. For the Wagenmakers data set, we want to calculate the density function for a model that has a threshold (`b`) parameter for each of the conditions (`cond` = `w`: 75% words & 25% non-words, `nw`: 75% non-words & 25% words). We also want threshold to vary for respone (`resp`) type  (i.e., the accumulator for a word (`W`) response, and the accumulator for the non-word (`NW`). So, on lines 16 and 17, we paste together the `"b."` threshold parameter, the condition (`cond` = `w` or `nw`) and the response `".W"` or `".NW"` and add the start point parameter `A`. The start point parameter is added to the threshold values to ensure that threshold is greater than the start point value. 

We hypothesised that drift rate `v` would vary with word frequency (`stim` = `hf`,`lf`,`vlf`,`nw`), so on lines 19 and 20 we allow drift rate for response <i>word</i> (`v.w`) to vary with the levels of word frequency, by pasting the `"v."` with `stim` and with the accumulator for each response (`".W"` or `".NW"`). You'll notice that in this example, we no longer have a drift rate for the correct response (`vc`) or incorrect response (`ve`), instead, we have a drift rate for responding <i>word</i> (`v.w`) and <i>non-word</i> (`v.nw`). This is a different way (and our preferred way) of coding drift rate. On line 21 we have ordered the `vs` vector with `v.nw` first and `v.w` second. 


```{r llSec1, attr.source = '.numberLines startFrom="14"',eval=FALSE}
 for (i in 1:nrow(data)) {
    A = x["A"]
    b.w = x[paste0("b.", data$prop[i], ".W")] + A
    b.nw = x[paste0("b.", data$prop[i], ".NW")] + A
    bs = list(b.nw, b.w)
    v.w = x[paste0("v.", data$freq[i], ".W")]
    v.nw = x[paste0("v.", data$freq[i], ".NW")]
    vs = c(v.nw, v.w)
    t0 = x["t0"]
    s = c(1, 1)
```
