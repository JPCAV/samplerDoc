# Example 3 - Linear Ballistic Accumulator - a more complex model.

```{r, required packages, include=FALSE}
library(tidyverse)
library(knitr)
library(devtools)
library(ggplot2)
library(pmwg)
```

```{css, echo=FALSE}
pre {
  max-height: 600px;
  overflow-y: auto;
}

pre[class] {
  max-height: 300px;
}
```

## Linear Ballistic Accumulator, a more complicated design example {.tabset .tabset-fade}

Previously, we demonstrated how the PMwG sampler can be used for a lexical decision task (LINK) in a signal detection framework to detect if there are ....???<!-- Question for Reilly, what was the purpose of this SDT example--> However, the signal detection framework fails to consider response time, and the join distribution in response time and accuracy. If you wanted to consider RT, you could instead use LBA model, instead of an SDT model. 

In this example, we will expand on what was covered in chapter 1, by fitting the LBA - a more complex model - to the wagenmakers 2008 data. <!-- see IS square paper experiment 2-->

A description of this experiment and data is covered in chapter 1 (LINK to chapter 1 description). This experiment has a more complex design than the Forstman (2008) example from the previous chapter, and the LBA is a more complex model than SDT. As a result, the loglikelihood function will be more complex, however, you'll notice that every step we take, is the same steps we followed in previous chapters with simpler data sets, or simpler model fits. 

First, we will make a log-likelihood function, perform some checks on the likelihood function and run the PMwG sampler. Following this we will generate posterior predictive data to look at goodness of fit, as well as use DIC as a model comparison method. Finally, we will look at chain convergence.


<!--
- These are data reilly fit in chap 1 with sig detection theory. When fit with signal detection, it was just prob. My fit with lba has to do a worse job of fitting probabilities, because it has to manage tnesion between fitting probs and rts. Adding rts has to give worse fit, but the tradeoff is that it allows us to look at the...-->


```{r part1, include = FALSE}
load("wagenmakers_2008.RData")
```

## Making the LBA LL_func 

Here we've created a slow, but easy to follow loglikelihood function. 

The bellow LBA function steps through the data line by line and gives a likelihood value for each line under x parameters. As per prior examples, we recommend people who have experience writing log likelihood write a faster likelihood function, or use the faster version we have created "HERE" <!--link-->, however this method is easier for people new to modelling, and is less likely to result in mistakes. 

Because most of the loglikelihood function follows the same pattern of previous examples, we will only step through the parts that differ, due to the different experimental design, and different hypothesis we have about which parameters are being influenced by these experimental manipulations.

```{r LL part2.1, include=TRUE }

library(rtdists)

```

```{r part2.2, attr.source = '.numberLines', include=TRUE }


trialwise_lba_loglike <- function(x, data, sample = FALSE) {
  x <- exp(x)
  if (any(data$rt < x["t0"])) {
    return(-1e10)
  }
  
  if (sample) {
    data$rt <- rep(NA, nrow(data))
    data$resp <- rep(NA, nrow(data))
  } else {
    out <- numeric(nrow(data))
  }
  
  for (i in 1:nrow(data)) {
    A = x["A"]
    b.w = x[paste0("b.", data$prop[i], ".W")] + A
    b.nw = x[paste0("b.", data$prop[i], ".NW")] + A
    bs = list(b.nw, b.w)
    v.w = x[paste0("v.", data$freq[i], ".W")]
    v.nw = x[paste0("v.", data$freq[i], ".NW")]
    vs = c(v.nw, v.w)
    t0 = x["t0"]
    s = c(1, 1)
    
    if (sample) {
      tmp <- rLBA(
        n = 1,
        A = A,
        b = bs,
        mean_v = vs,
        sd_v = s,
        t0 = t0,
        dist = "norm",
        silent = TRUE
      )
      data$rt[i] <- tmp$rt
      data$resp[i] <- tmp$resp
    } else {
      out[i] <-
        dLBA(
          rt = data$rt[i],
          response = data$resp[i],
          A = A,
          b = bs,
          mean_v = vs,
          sd_v = s,
          t0 = t0,
          dist = "norm",
          silent = TRUE
        )
    }
  }
  if (sample) {
    return(data)
  } else {
    bad <- (out < 1e-10) | (!is.finite(out))
    out[bad] <- 1e-10
    out <- sum(log(out))
    return(out)
  }
}

```

Similar to our previous trialwise likelihood function in chapter 2, we do a for loop for each row. In this loop, we assign the parameter values (x) to each parameter in our model. 

Since we want to calculate the density function for a model that has two threshold parameters for each condition in the `proportion factor` (`W`: 75% stimuli words, `NW`: 75% stimuli nonwords), but also allows threshold to vary with `response` (i.e., the accumulator for a `word` response, and the accumulator for the `non-word` response), we include lines 14 - 23. These lines take into account the two conditions in the `proportion` factor by pasting `"b."`, the `condition` (`W`, `NW`) that is on line [i], and whether it is a threshold for the <i>word accumulator</i> `".W"` or <i>nonword accumulator</i> `".NW"`. Again, we add the sart point parameter `A` value to each threshold parameter so that threshold is greater than the start point value. 

We also hypothesised that <i> drift rate</i>  would vary with `word frequency` (`hf`,`lf`,`vlf`,`nw`), so on lines 19 and 20 we allow drift rate for response `word` (`v.w`) to vary with the levels of word frequency, by pasting the `"v."` with the word frequency on line [i], and with the accumulator (`".W"` or `".NW"`). You'll notice that in this example, we no longer have a drift rate for the correct response (vc) or incorrect response (ve), instead, we have a drift rate for responding `word` (`v.w`) and `non-word` (`v.nw`). This is just a different way of coding drift rate, and is our preferred way.
On line x we have coded the vs vector to have v.nw first and v.w second. 

```{r part2.3, attr.source = '.numberLines startFrom="14"',eval=FALSE}

 for (i in 1:nrow(data)) {
    A = x["A"]
    b.w = x[paste0("b.", data$prop[i], ".W")] + A
    b.nw = x[paste0("b.", data$prop[i], ".NW")] + A
    bs = list(b.nw, b.w)
    v.w = x[paste0("v.", data$freq[i], ".W")]
    v.nw = x[paste0("v.", data$freq[i], ".NW")]
    vs = c(v.nw, v.w)
    t0 = x["t0"]
    s = c(1, 1)
      
```


It is important to check the ordering of your levels for the response factor, to ensure you correctly order your `v.nw` and `v.w` accumulators in the 'vs' vector.  

If we look at our levels for the responses, we see that `NW` is <i>level 1</i> and `W` is <i>level 2</i>. This means that `NW` is coded as `1` and `W` is coded as `2`. Therefore, from line 32, if data [dollar] resp[i] = N.W (<i>Non-word response</i>), i.e., level 1, the first accumulator in position 1 will be chosen, and if data [dollar] resp[i] = W (<i>Word response</i>), this tells the sampler that the accumulator in position 2 will be chosen. Because in our `vs` vector (Line 21) we put `v.nw` in position 1, and `v.w` in position 2, these will map correctly.


```{r part2.4, include=TRUE }

levels(data$resp)
      
```

The rest of the loglikelihood is identical to the one used in the chapter 2 Forstmann example, except now we set `b` to equal the list we created for <i>threshold</i> (`b = bs`). This can be seen on lines 29 and 44.


```{r part2.7, attr.source = '.numberLines startFrom="25"', eval=FALSE} 

    if (sample) {
      tmp <- rLBA(
        n = 1,
        A = A,
        b = bs,
        mean_v = vs,
        sd_v = s,
        t0 = t0,
        dist = "norm",
        silent = TRUE
      )
      data$rt[i] <- tmp$rt
      data$resp[i] <- tmp$resp
    } else {
      out[i] <-
        dLBA(
          rt = data$rt[i],
          response = data$resp[i],
          A = A,
          b = bs,
          mean_v = vs,
          sd_v = s,
          t0 = t0,
          dist = "norm",
          silent = TRUE
        )
    }
  }
  if (sample) {
    return(data)
  } else {
    bad <- (out < 1e-10) | (!is.finite(out))
    out[bad] <- 1e-10
    out <- sum(log(out))
    return(out)
  }
}
```


## Fast LBA_LL

Again, it is important to note that this is a very slow and computationally inneficient way of writing a log likelihood. Below we have provided a faster loglikelihood function that only calls dLBA once when sample = FALSE.


```{r fast_lba,include=TRUE}

fast_lba_loglike <- function(x, data, sample = FALSE) {
  x <- exp(x)
  if (any(data$rt < x["t0"])) {
    return(-1e10)
  }
  
  if (sample) {
    data$rt <- rep(NA, nrow(data))
    data$resp <- rep(NA, nrow(data))
  } else {
    out <- numeric(nrow(data))
  }
   if (sample) {
    for (i in 1:nrow(data)) {
    A = x["A"]
    b.w = x[paste0("b.", data$prop[i], ".W")] + A
    b.nw = x[paste0("b.", data$prop[i], ".NW")] + A
    bs = list(b.nw, b.w)
    v.w = x[paste0("v.", data$freq[i], ".W")]
    v.nw = x[paste0("v.", data$freq[i], ".NW")]
    vs = c(v.nw, v.w)
    t0 = x["t0"]
    s = c(1, 1)
    
      tmp <- rLBA(
        n = 1,
        A = A,
        b = bs,
        mean_v = vs,
        sd_v = s,
        t0 = t0,
        dist = "norm",
        silent = TRUE
      )
      data$rt[i] <- tmp$rt
      data$resp[i] <- tmp$resp
    }
  } else {
  
    prop = levels(data$prop)
    freq = levels(data$freq)

    blist = list("b.nw" = numeric(nrow(data)), "b.w" = numeric(nrow(data)))
    vlist = list("v.nw" = numeric(nrow(data)), "v.w" = numeric(nrow(data)))


    for (p in prop) {
      for (f in freq) {
          use <- data$prop == p & data$freq == f 
          if (any(use)) {
            b.w = x[paste0("b.", p, ".W")] + x["A"]
            b.nw = x[paste0("b.", p, ".NW")] + x["A"]
            v.w = x[paste0("v.", f, ".W")]
            v.nw = x[paste0("v.", f, ".NW")]
            blist$b.nw[use] = b.nw
            blist$b.w[use] = b.w
            vlist$v.nw[use] = v.nw
            vlist$v.w[use] = v.w
          }
      }
    }

    out <- dLBA(
      rt = data$rt,
      resp = data$resp,
      A = x["A"],
      b =  blist,
      t0 = x["t0"],
      mean_v = vlist,
      sd_v = c(1, 1),
      distribution = "norm",
      silent = TRUE
    )
  }
 

  if (sample) {
    return(data)
  } else {
    bad <- (out < 1e-10) | (!is.finite(out))
    out[bad] <- 1e-10
    out <- sum(log(out))
    return(out)
  }
  }

```


## Trouble shooting the loglikelihood function

Again, we can run some simple tests to see if there are any errors in internal consistency with our loglikelihood function. 

I've assigned somewhat sensible x values, since I expect the threshold for the word accumulator to be lower than the threhold for the non-word accumulator, when 75% of the stimuli are words, and the opposite when 75% of the stimuli are non-words. I also expect people to have a much larger drift for the word accumulator when the stimului are high frequency words. These are just some of the assumptions ive made in my chosen values bellow. 

Line (p) will give me a value for my log-likelihood from the function. 

```{r part3.1, include=TRUE}

x = log(c("A"=2,"b.nw.NW"=1,"b.nw.W"=2,"b.w.NW"=2.5,"b.w.W"=.5,"v.hf.NW"=1.5,"v.hf.W"=5,"v.lf.NW"=2.5,"v.lf.W"=3.5,"v.vlf.NW"=2,"v.vlf.W"=3,"v.nw.NW"=5,"v.nw.W"=2,"t0"=.19))

trialwise_lba_loglike(x,data,sample=FALSE)
fast_lba_loglike(x,data,sample = FALSE)

```

Now when we change the values, we should see a change in the log-likelihood. Here, for brevity, I've just changed my `b` values, but you can test each parameter individually by changing its value and seeing if this affects the loglikelihood that is returned.

```{r part3.2, include=TRUE}
x = log(c("A"=2,"b.nw.NW"=3,"b.nw.W"=1,"b.w.NW"=1.5,"b.w.W"=2,"v.hf.NW"=1.5,"v.hf.W"=5,"v.lf.NW"=2.5,"v.lf.W"=3.5,"v.vlf.NW"=2,"v.vlf.W"=3,"v.nw.NW"=5,"v.nw.W"=2,"t0"=.19))
trialwise_lba_loglike(x,data)
fast_lba_loglike(x,data,sample = FALSE)
```

This absolute value is larger than the previous one (further away from zero), so these values are less accurate given the data. 

Notice how both my trialwise (slow) and fast loglikelihoods produce the same result. I can also use this as a way of checking my more complicated fast loglikelihood function against my slower but simpler function.

A more comprehensive way to test your log-likelihood function is to generate fake data for which you know the true data generating values. This is a form of 'parameter recovery', where you run the sampler on generated data and plot the likelihood change as x values move away from your 'true' x-value. If the true-x has the highest likelihood, then it is likely that your function is working as intended. 

The steps here are identical to that in the previous Forstmann example. First we generate fake data for which we know the true data generating values. Ensure these are sensible values. For example, the drift rate for the the `hf.W` accumulator should be higher than the `nw.W` accumulator, and the value given to `t0` should not be bigger than the smallest respones time [min(data$rt)] in the original data set. 

```{r part4.1,include=TRUE}
true.x <- log(c("A"=2,"b.nw.NW"=1,"b.nw.W"=2,"b.w.NW"=2.5,"b.w.W"=.5,"v.hf.NW"=1.5,"v.hf.W"=5,"v.lf.NW"=2.5,"v.lf.W"=3.5,"v.vlf.NW"=2,"v.vlf.W"=3,"v.nw.NW"=5,"v.nw.W"=2,"t0"=.19))
```

We now use these <i>"true"</i> values to generate fake data by setting x = true.x and `sample = TRUE`. Making data = data will ensure the fake data is the length of the original data set. Since this is a big data set, I'll make a smaller data-set to save some computational time. 

```{r part4.2,include=TRUE}
test.data = trialwise_lba_loglike(x=true.x,data=data,sample=TRUE)
little.test.data = head(test.data,n=10000)
```

Now we can generate profile plots that compare the log likelihood of the true.x values against nearby values. You can write your own code or source the bellow function to do this. See previous Forstmann example for more detailed notes on the arguments for this function.

```{r part4.3,include=TRUE}

#install.packages("parallel")
library(parallel)


test.likelihood <- function(x,num.values=9,fake.data,dimensions,ll_func,server=FALSE,...) {
  pars = names(x)
  x.tmp = x
  sequence <- seq(from =-.22, to = .22, length.out = num.values)
  op <- par(mfrow = dimensions)
  par_likelihoods = lapply(setNames(pars,pars),function(p){
    testvalues = sequence+true.x[[p]]
    if (server) {
      tmp <- unlist(mclapply(testvalues,function (i){ #for each test vlaue, it will apply the function where i = testvalue.
      x.tmp[[p]] = i
      ll_func(x=x.tmp,data=fake.data)
    },mc.cores = num.values))
    } else {
      tmp <- unlist(mclapply(testvalues,function (i){ #for each test vlaue, it will apply the function where i = testvalue.
      x.tmp[[p]] = i
      ll_func(x=x.tmp,data=fake.data)
    },mc.cores = 1))
    }
    plot(x=testvalues,
         y=tmp,
         type="b",
         main= p,
         xlab = "log par values",
         ylab = "loglikelihood",
         ...)
    abline(v = true.x[[p]], col = "red")
    return(tmp)})
  par(op)
  return(par_likelihoods)
}

```

Whilst we used the trialwise likelihood function to generate data, we will use the fast likelihood function in this mini "parameter recovery" for computational efficiency. 

I have 14 parameters ("A","b.nw.NW","b.nw.W","b.w.NW","b.w.W","v.hf.NW","v.hf.W","v.lf.NW","v.lf.W","v.vlf.NW","v.vlf.W","v.nw.NW","v.nw.W","t0") so I will set my dimensions to be five rows with three columns.

NOTE: Before running this function ensure the plotting space is maximised, otherwise you will get an error about plotting margins. 
```{r part4.4,include=TRUE}
likelihood_plots = test.likelihood(x= true.x,num.values=9,fake.data = little.test.data,dimensions = c(3,5),ll_func=fast_lba_loglike) 
```

The red line indicates the true generating parameter value.

Note: These plots won't tell you if your log-likelihood is correct in the sense that you've written a log-likelihood for the model you actually want to test, however it will tell you whether the function is working in the way you expect it would - with the fata generation function matching the density function.

Below is an example output from a function where there is an error.

```{r part4.5,include=FALSE}


incorrect_lba_loglike <- function(x, data, sample = FALSE) {
  x <- exp(x)
  if (any(data$rt < x["t0"])) {
    return(-1e10)
  }
  
  
  if (sample) {
    for (i in 1:nrow(data)) {
      A = x["A"]
      b.w = x[paste0("b.", data$prop[i], ".W")] + A
      b.nw = x[paste0("b.", data$prop[i], ".NW")] + A
      bs = list(b.nw, b.w)
      v.w = x[paste0("v.", data$freq[i], ".W")]
      v.nw = x[paste0("v.", data$freq[i], ".NW")]
      vs = c(v.nw, v.w)
      t0 = x["t0"]
      s = c(1, 1)
      
      tmp <- rLBA(
        n = 1,
        A = A,
        b = bs,
        mean_v = vs,
        sd_v = s,
        t0 = t0,
        dist = "norm",
        silent = TRUE
      )
      data$rt[i] <- tmp$rt
      data$resp[i] <- tmp$resp
    }
  } else {
    prop = levels(data$prop)
    freq = levels(data$freq)
    resp = levels(data$resp)
    
    blist = list("b.nw" = numeric(nrow(data)), "b.w" = numeric(nrow(data)))
    vlist = list("v.nw" = numeric(nrow(data)), "v.w" = numeric(nrow(data)))
    
    
    for (p in prop) {
      for (f in freq) {
        for (r in resp) {
          use <- data$prop == p & data$freq == f & data$resp == r
          if (any(use)) {
            b.w = x[paste0("b.", p, ".W")] + x["A"]
            b.nw = x[paste0("b.", p, ".NW")] + x["A"]
            v.w = x[paste0("v.", f, ".W")]
            v.nw = x[paste0("v.", f, ".NW")]
            blist$b.nw[use] = b.nw
            blist$b.w[use] = b.w
            vlist$v.nw[use] = v.nw
            vlist$v.w[use] = v.w
          }
        }
      }
    }
    
    
    out <- numeric(nrow(data))
    out <- dLBA(
      rt = data$rt,
      resp = data$resp,
      A = x["A"],
      b =  blist,
      t0 = x["t0"],
      mean_v = vlist,
      sd_v = c(1, 1),
      distribution = "norm",
      silent = TRUE
    )
  }
  
  if (sample) {
    return(data)
  } else {
    bad <- (out < 1e-10) | (!is.finite(out))
    out[bad] <- 1e-10
    out <- sum(log(out))
    return(out)
  }
}


```

```{r part4.6, echo=FALSE}
likelihood_plots = test.likelihood(x= true.x,num.values=9,fake.data = little.test.data,dimensions = c(2,4),ll_func=incorrect_lba_loglike) 
```


## PMwG Framework - running the sampler using the LBA

Now that we have written a log likelihood function, we can use the PMwG sampler package. Running the sampler follows the same process as the previous chapters - the only change is in the likelihood function, the parameter vector and start points/priors. 

First load your libraries and data.
```{r part7.1, include = TRUE}
rm(list=ls())
library(psamplers)
library(rtdists)
```

```{r part7.2, include = FALSE}
load("data/wagenmakers2008.RData")
```

Now create a vector of your model parameter names, which must match the names and number of parameters you include in your loglikelihood function.You can name this vector as you wish; however, in our example, we will call it `pars`. 

For this dataset, we use four threshold parameters, one for each combination of the `proportion` (75% word stimuli vs. 75% non-word stimuli), and response (repsonding Word vs responding non-word) because we assume that both proportion and response have an effect on level of caution. 

We also hypothesised that <i> drift rate</i>  would vary with `word frequency` (`hf`,`lf`,`vlf`,`nw`), so we have 8 drift rates (four for each level of word frequency in the Word response accumulator `.W`, and four for level of word frequency in the non-word response accumulator `.NW`).

We've made a decision to set the `sv` to 1 to satisfy the scaling properties of the model, as such we haven't included the `sv` parameter in the `pars` vector - it is found in the LBA's likelihood function (see bellow).

```{r part7.3, include = TRUE}

pars = c("A","b.nw.NW","b.nw.W","b.w.NW","b.w.W","v.hf.NW","v.hf.W","v.lf.NW","v.lf.W","v.vlf.NW","v.vlf.W","v.nw.NW","v.nw.W","t0")

```

Next we create a `priors` object; a list that contains two components:

* `theta_mu` a vector containing the prior for model parameter means
* `theta_sig` the prior covariance matrix for model parameters.

<!-- RJI: might put a note on this one about setting mu lower than 1 and why we use 0s in the off diagonal -->
The `priors mu` object in our example is initiated with zeros, and the covariance has the diagonal of 1 and 0's for the off diagonal. We use generic priors here which are broad. This means that the prior is <b>blah</b> and the covariance matrix posterior space is able to be explored. In some examples, the prior will need to be adjusted to account for different factors or priors we already know. 

```{r part7.3, include = TRUE}

priors <- list(
  theta_mu = rep(0, length(pars)),
  theta_sig = diag(rep(1, length(pars)))
)

```

Next, specify your loglikelihood function (ll_func). We use the fast version we wrote; 

```{r part7.4, include = TRUE}



fast_lba_loglike <- function(x, data, sample = FALSE) {
  x <- exp(x)
  if (any(data$rt < x["t0"])) {
    return(-1e10)
  }
  
  if (sample) {
    data$rt <- rep(NA, nrow(data))
    data$resp <- rep(NA, nrow(data))
  } else {
    out <- numeric(nrow(data))
  }
   if (sample) {
    for (i in 1:nrow(data)) {
    A = x["A"]
    b.w = x[paste0("b.", data$prop[i], ".W")] + A
    b.nw = x[paste0("b.", data$prop[i], ".NW")] + A
    bs = list(b.nw, b.w)
    v.w = x[paste0("v.", data$freq[i], ".W")]
    v.nw = x[paste0("v.", data$freq[i], ".NW")]
    vs = c(v.nw, v.w)
    t0 = x["t0"]
    s = c(1, 1)
    
      tmp <- rLBA(
        n = 1,
        A = A,
        b = bs,
        mean_v = vs,
        sd_v = s,
        t0 = t0,
        dist = "norm",
        silent = TRUE
      )
      data$rt[i] <- tmp$rt
      data$resp[i] <- tmp$resp
    }
  } else {
  
    prop = levels(data$prop)
    freq = levels(data$freq)

    blist = list("b.nw" = numeric(nrow(data)), "b.w" = numeric(nrow(data)))
    vlist = list("v.nw" = numeric(nrow(data)), "v.w" = numeric(nrow(data)))


    for (p in prop) {
      for (f in freq) {
          use <- data$prop == p & data$freq == f 
          if (any(use)) {
            b.w = x[paste0("b.", p, ".W")] + x["A"]
            b.nw = x[paste0("b.", p, ".NW")] + x["A"]
            v.w = x[paste0("v.", f, ".W")]
            v.nw = x[paste0("v.", f, ".NW")]
            blist$b.nw[use] = b.nw
            blist$b.w[use] = b.w
            vlist$v.nw[use] = v.nw
            vlist$v.w[use] = v.w
          }
      }
    }

    out <- dLBA(
      rt = data$rt,
      resp = data$resp,
      A = x["A"],
      b =  blist,
      t0 = x["t0"],
      mean_v = vlist,
      sd_v = c(1, 1),
      distribution = "norm",
      silent = TRUE
    )
  }
 

  if (sample) {
    return(data)
  } else {
    bad <- (out < 1e-10) | (!is.finite(out))
    out[bad] <- 1e-10
    out <- sum(log(out))
    return(out)
  }
  }
  
```

Now, we create the PMwG `sampler` object, including the factors specified above. 


The `pmwgs` function takes a set of arguments (listed below) and returns a list containing the required components for performing the particle metropolis within Gibbs steps.

* `data =` your data - a data frame (e.g.`data` from the loaded wagenmakers2008 RData file) with a column for participants called <b>`subject`</b>
* `pars =` the model parameters to be used (e.g.`pars`)
* `prior =` the priors to be used (e.g.`priors`)
* `ll_func =` name of log likelihood function you've sourced above (e.g.`fast_lba_loglike`)


```{r part7.5, include = TRUE}
sampler <- pmwgs(
  data = data,
  pars = pars,
  prior = priors,
  ll_func = fast_lba_loglike
)
```



Following this, we run the sampling arguments. First we use the `init` function to generate initial start points for the random effects and storing them in the `sampler` object. If you wanted you could include start points for your chain. We will not specify startpoints as we did in the Forstmann three threshold model. This is why have not included a `start_mu` and a `start_sig`. Instead, the default is that start points are drawn from the prior. 

```{r part7.6, include = TRUE}
sampler <- init(sampler) 
```

Now we can run the sampler using the `run_stage` function. The `run_stage` function takes four arguments:

* `x =` the `sampler` object including parameters that was created from the `init` function above.
* `stage =` the sampling stage (These are:`"burn"`, `"adapt"` or `"sample"` - in this order).
* `iter =` is the number of iterations for the sampling stage. This is similar to running deMCMC, where it takes many iterations to reach the posterior space. Default = 1000.
* `particles =` is the number of particles generated on each iteration. Default = 1000.
* `epsilon =` is a value between 0 and 1 which reduces the size of the sampling space. We use lower values of epsilon when there are more parameters to estimate. Default = 1.
* `n_cores =` the number of cores on a machine you wish to use to run the sammpler. This allows sampling to be run across cores (parallelising for subjects). Default = 1. 

Since this is a large model with a lot of parameters, we've increased the number of particles to 10000 in burn in and adapted stages, and set epsilon to equal .1. This helps ensure an adequate amount of samples are being accepted across each stage. 

NOTE: Overall, we aim for ~30% acceptance rate of particles. Acceptance rates which are too high may be inaccurate if not run for long enough. Acceptance rates which are too low will be inefficient and may fail to create an efficient distribution for the sampling stage. Due to this being a data set with very low number of errors, it is difficult to sample. As a result, we have not achieved the 30% acceptance rate, with sampling being somewhat inefficient.

To run burn-in, set the stage argument to "burn". To run the adaptation stage, set the stage argument to "adapt", and the run the sampled stage, set the stage argument to "sample"

```{r part7.7, eval = FALSE}
burned <- run_stage(sampler, stage = "burn",iter=1000,particles=10000,epsilon=.3)
#burned <- run_stage(sampler, stage = "burn",iter=3000,particles=1000,n_cores = 19)

adapted <- run_stage(burned, stage = "adapt",iter=10000,particles=10000,epsilon=.3)

sampled <- run_stage(adapted, stage = "sample",iter=1000,particles=1000,epsilon=.3)

```

Save your output.

```{r part7.8, eval = FALSE }
save.image("data/output/wagenmakers2008_sampled.RData")

```

## Simulating Posterior Data


To generate posterior predictive data we use the `SAMPLE = TRUE` part of our likelihood function we used above. This part of the likelihood function stores the generated rt and responses given the posterior parameter estimates. 

Below we have included code you can use to simulate posterior predictive data given  the `sampled` object is loaded in your work environment. `n` specifies the number of posterior samples. To generate this you will to load your `sampled` object for your model.

```{r part8.1, Include=TRUE}

generate.ppdata <- function (sampled,n,ll_func = sampled$ll_func,rbind.data = TRUE) {
  index = which(sampled$samples$stage=="sample")
  iterations=round(seq(from=index[1],to=index[length(index)],length.out=n))
  data = sampled$data
  S = sampled$n_subjects
  pp.data=list()
    for (s in 1:S){
       print(paste0("subject",s))
      for (i in 1:length(iterations)) {
       print(i)
        x <- sampled$samples$alpha[,s,iterations[i]]
        names(x) <- pars
        out <- ll_func(x=x,data=data[data$subject==s,],sample=TRUE)
        if (i==1){
          pp.data[[s]] = cbind(pp.subject=i,out)
        }
        else {
          pp.data[[s]]= rbind(pp.data[[s]],cbind(pp.subject=i,out))
        }
      }
    }
    if (rbind.data){
    tidy.pp.data=do.call(rbind,pp.data)  
    return(tidy.pp.data)
    }
    else {
      return(pp.data)
    }
}


```

```{r part8.2, include=FALSE}
library(rtdists)
load("wagenmakers2008_sampled.RData")

```

We generate 20 posterior predictive data samples for each model. 
```{r part8.3, eval=FALSE}
pp.data = generate.ppdata(sampled,n=20)
```


```{r part8.4, include=FALSE}
load("ppdata.RData")
library(tidyverse)
```

The returned data is a matrix with the same dimensions and names as data -- with the exception of column `"pp.subject"`. `pp.subject` is the iteration of posterior sample (in this example i = 1:20) for the corresponding subject. We now have two matrices based on samples from either model. "The response (resp) and response time (rt) columns are now replaced with the posterior predictive data.

```{r part8.5, echo=FALSE}
head(pp.data)
```

In the next section, we will use this to assess descriptive adequacy.

## Assessing Descriptive Adequacy (goodness of fit)

Here we use tidyverse to plot the posterior predictive data against the real data. You may choose to plot this in other ways such as the inter-quantile range or via other packages. 

This may only be useful for some, as what you do with your data is domain specific knowledge. 

```{r part9.1, include=TRUE,message=FALSE,warning=FALSE}
 library(tidyverse)
```

```{r part9.2, include=TRUE,warning=FALSE,message=FALSE}

  qL=function(x) quantile(x,prob=0.25,na.rm=T)
    qH=function(x) quantile(x,prob=0.75,na.rm=T)
    medianNA=function(x) median(x,na.rm=T)

    #Create new column for stimulus that is just W vs NW
    
    data$stim = ifelse(data$freq=="nw","NW","W")
    pp.data.E2$resp = factor(pp.data.E2$resp,levels=c("1","2"),labels=c("NW","W"))
    pp.data.E2$stim = ifelse(pp.data.E2$freq=="nw","NW","W")
    
  pq = data %>% group_by(prop,freq,subject) %>% summarise_at(vars(rt), funs(qL, medianNA,qH))
    tmp = data %>% group_by(prop,freq,subject) %>% summarise(acc=mean(resp==stim))
    pq=cbind(pq,acc=tmp$acc)
    
  pp.pq = pp.data.E2 %>% group_by(prop,freq,pp.subject,subject) %>% summarise_at(vars(rt), funs(qL, medianNA,qH))
    tmp = pp.data.E2 %>% group_by(prop,freq,pp.subject,subject) %>% summarise(acc=mean(resp==stim))
    pp.pq=cbind(pp.pq,acc=tmp$acc)

  pq=bind_rows(cbind(src=rep("data",nrow(pq)),pq),cbind(src=rep("model",nrow(pp.pq)),pp.pq))



  ## Averages to plot.
  av.pq = pq %>% group_by(src,prop,freq) %>% summarise_at(vars(qL:acc),mean)
  ## Variances of posterior samples.
  post.var = pq %>% filter(src!="data") %>% group_by(prop,freq,pp.subject,src) %>% summarise_at(vars(qL:acc),mean)

  ## These make the plots better:
  av.pq$src=factor(av.pq$src,levels=c("model","data"),labels=c("model","(Data)"))
  post.var$src=factor(post.var$src,levels=c("model","data"),labels=c("model","(Data)"))

  ## Change some level labels, for pretty plotting.
  av.pq$prop=factor(av.pq$prop,levels=c("nw","w"))
  post.var$prop=factor(post.var$prop,levels=c("nw","w"))
    
  levels(av.pq$prop)<-levels(post.var$prop)<-c("75% Non-Word stimuli","75% Word stimuli")
  
  ## Make for msec and percentages.
  av.pq$acc=100*av.pq$acc
  post.var$acc=100*post.var$acc
  av.pq[,c("qL","medianNA","qH")]=1000*av.pq[,c("qL","medianNA","qH")]
  post.var[,c("qL","medianNA","qH")]=1000*post.var[,c("qL","medianNA","qH")]
  
##response time plots
rtplot=ggplot(data=av.pq,mapping=aes(x=freq,y=medianNA,group=src,colour=src))+geom_point(size=4)+facet_wrap(~prop)


rtplot=rtplot + geom_point(aes(x=freq,y=qL,color=src,group=src),size=2)
                           
rtplot=rtplot + geom_point(aes(x=freq,y=qH,color=src,group=src),size=2)

rtplot = rtplot + labs(x="Word Condition",y="RT Percentiles (ms.)",colour="Model")


rtplot=rtplot + geom_point(data=post.var,aes(x=freq,y=qL,group=src),size=1,color="black",alpha=0.2)
rtplot=rtplot + geom_point(data=post.var,aes(x=freq,y=qH,group=src),size=1,color="black",alpha=0.2)
rtplot=rtplot + geom_point(data=post.var,aes(x=freq,y=medianNA,group=src),size=2,color="black",alpha=0.2)

rtplot = rtplot + guides(colour=FALSE)



##accuracy plots
accplot=ggplot(data=av.pq,mapping=aes(x=freq,y=acc,color=src,group=src))+geom_point(size=4)+facet_wrap(~prop)

accplot = accplot + geom_point(data=post.var,aes(x=freq,y=acc,group=src),color="black",alpha=.2)

accplot = accplot + labs(x="Word Condition",y="Mean Accuracy (%)",colour="Model")
accplot = accplot + guides(colour=FALSE)


##Join plots
library(gridExtra)
library(grid)

gA=ggplotGrob(rtplot)
gB=ggplotGrob(accplot)
maxWidth = grid::unit.pmax(gA$widths[2:5],gB$widths[2:5])
gA$widths[2:5]<-as.list(maxWidth)
gB$widths[2:5]<-as.list(maxWidth)
bothplot = grid.arrange(gA,gB,ncol=1)

plot(bothplot)
#ggsave(file="modelfit-nolegend.pdf",width=7,height=4.5,plot=bothplot)

#write_csv(x=av.pq,path="numbers-from-the-graph.csv")

```

## Calculating DIC

You can use the deviance information criterion (DIC) as a metric to compare two models, and assess which model better fits the data, with penalties applied to model complexity. 

Whilst we have not created two models to compare in this example, we have still included the function, and output DIC value for completeness. To see an example where DIC is used as a model selection technique, see chapter 2 <!--(link to this section)-->

```{r part10.1, include=TRUE}
library(pmwg)
pmwg_DIC=function(sampled){
nsubj=length(unique(sampled$data$subject))

# the mean likelihood of the overall (sampled-stage) model, separately for each subject
mean.like <- apply(sampled$samples$subj_ll[,sampled$samples$stage=="sample"],1,mean)

# the mean of each parameter across iterations. Keep dimensions for parameters and subjects
mean.params <- t(apply(sampled$samples$alpha,1:2,mean))

# i name mean.params here so it can be used by the log_like function
colnames(mean.params)<-sampled$par_names

# log-likelihood for each subject using their mean parameter vector
mean.params.like <- numeric(ncol(mean.params))
for (j in 1:nsubj) {
  mean.params.like[j] <- sampled$ll_func(mean.params[j,], data=sampled$data[sampled$data$subject==j,], sample=FALSE)
}

# Effective number of parameters
pD <- sum(-2*mean.like + 2*mean.params.like)

# Deviance Information Criterion
DIC <- sum(-4*mean.like + 2*mean.params.like)

return(c("DIC"=DIC,"effective parameters"=pD))
}
```

Now using this function we can then check the DIC value for the sampled object. 

```{r part10.2, include=TRUE}
load("wagenmakers2008_sampled.RData")
```

```{r part10.3, include=TRUE}
pmwg_DIC(sampled)
```

## Checking Convergence. 

As parameters are concurrently estimated, there is no 'convergence' to observe, at least in the same sense as deMCMC. Rather, it is best to check for stationarity of parameters. We can check for convergence if we are uncertain of reasonable parameter values and to be sure of our estimates. 

To check for convergence, we run multiple 'chains' (runs) of the sampler, and check if these chains converge. To do this, we repeat the above steps, being sure to save the output of each chain under a different name. In this example we run the sampler 3 times, not specifying start points. You may want to vary the start points or priors to check convergence as well. You could run this process by running the same script n.chains times <!-- or you could run the sampling stages each n.chains times and saving these as different names (for example, we would call our objects burned.1, burned.2 etc). you need different start points so running different burns/adapted wouldnt work-->. In this example, we run them three separate times and save the output as different names. 

```{r part11.1,include=TRUE}
rm(list=ls())
```

We use the below function to get the names for each chain to operate on. 
```{r part11.2,include=TRUE}

list.chains <- function (location,name,samplestage="sampled") {
  chain.names = dir(path = location, pattern = name) 
  tmp = vector("list",length(chain.names))
  for (i in seq_along(chain.names)) {
    load(paste0(location,"/",chain.names[i]))
    tmp[[i]] = get(samplestage)
  }
  return(tmp)
}
```

Bellow illustrates the location we saved our outputs, and how we named our sample chains. Your path/name will likely be different.
```{r part11.3,eval=FALSE}

chains = list.chains(location = "data/output",name = "sampled_chain",samplestage="sampled")

```

```{r part11.4,include=FALSE}

chains = list.chains(location = "~/Desktop/samplerDoc_caro",name = "sampled_chain",samplestage="sampled")

```
<!-- make sure this is different from forstmann one-->

Using the coda and mcmcplots package, we provide a function which gathers samples from the chains in a single object. 

```{r part11.5,include=TRUE}

library(coda)
library(mcmcplots)

as_mcmc = function(x) {
  theta_mu = (t(x$samples$theta_mu[,x$samples$stage=="sample"]))
  colnames(theta_mu) = paste0("mu_", x$par_names)
  
  par_names <- x$par_names
  sig_names <- matrix(nrow = length(par_names), ncol = length(par_names))
  for (i in seq_along(par_names)) {
    for (j in seq_along(par_names)) {
      sig_names[i,j] <- paste0("sig[", par_names[i], ",", par_names[j], "]")
    }
  }
  theta_sig <- t(apply(x$samples$theta_sig[,,x$samples$stage=="sample"], 3, as.vector))
  colnames(theta_sig) <- as.vector(sig_names)
  
  S = x$n_subjects
  alpha_names <- matrix(nrow = length(par_names), ncol = S)
  for (i in seq_along(par_names)) {
    for (j in seq_len(S)) {
      alpha_names[i,j] <- paste0("alpha[", par_names[i], ",","sub",j, "]")
    }
  }
  alpha <- t(apply(x$samples$alpha[,,x$samples$stage=="sample"], 3, as.vector))
  colnames(alpha) <- as.vector(alpha_names)
  
  tmp2 <- cbind(theta_mu,theta_sig,alpha)
  return(mcmc(tmp2))
}

create.mcmc.list <- function(x) {
  mcmc.list(lapply(x,as_mcmc))
}

```

We call the above function to create our mcmc lists.
```{r part11.6,include=TRUE}

test_lists = create.mcmc.list(chains)

```

Next, we create plots (using the coda and mcmcplots packages) to check convergence.

```{r part11.7,eval=FALSE}

mcmcplot(test_lists)

```


```{r part11.8,include=FALSE}

mcmcplot(test_lists)

```

Bellow we've included a snipped of the generated MCMC plots. When viewing these, you want to check that... (ASK SCOTT for WORDING)
```{r part11.9,echo=FALSE,out.width='100%', fig.show='hold', fig.cap='Example of output of MCMC plot function when used with 3 chains.'}
knitr::include_graphics('_images/MCMCplotexample.PNG')
```

In another test of convergence, we use the gelman.diag function to produce an r hat value (similar to that used in rjags).

NOTE: Approximate convergence is diagnosed when rhat values are less than 1.

```{r part11.10,eval=FALSE}

gelman.diag(test_lists,multivariate = FALSE)

```

Bellow we just show the head of the gelman.diag function,

```{r part11.11,echo=FALSE,eval=TRUE}

tmp = gelman.diag(test_lists,multivariate = FALSE)
tmp2 = head(tmp$psrf)
print(tmp2)

```

You may also wish to use the following lines of code to find the maximum rhat value. 

```{r part11.12,eval = FALSE}

apply(gelman.diag(test_lists,multivariate = FALSE)[[1]],2,max)
apply(gelman.diag(test_lists,multivariate = FALSE)[[1]],2,which.max)


```


```{r part11.13,echo=FALSE,eval=TRUE}

tmp1=apply(gelman.diag(test_lists,multivariate = FALSE)[[1]],2,max)
tmp2=apply(gelman.diag(test_lists,multivariate = FALSE)[[1]],2,which.max)
print(tmp1)
print(tmp2)

```
